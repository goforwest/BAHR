# ğŸ¼ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠ (Prosody Engine)
## Ø§Ù„ØªÙˆØ«ÙŠÙ‚ Ø§Ù„ØªÙ‚Ù†ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ â€“ Ù†Ø³Ø®Ø© Ù…Ø±Ø§Ø¬Ø¹Ø© Ù‡Ù†Ø¯Ø³ÙŠØ©

**Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«:** 8 Ù†ÙˆÙÙ…Ø¨Ø± 2025 (ØªÙ†Ù‚ÙŠØ­ Ø´Ø§Ù…Ù„ ÙˆÙÙ‚ Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙ‚Ù†ÙŠØ©: ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§ØªØŒ Ø¥Ø¨Ø±Ø§Ø² Ø­Ø§Ù„Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ØŒ ÙØµÙ„ Ø§Ù„Ù…Ø±Ø­Ù„ÙŠ Ø¹Ù† Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØŒ Ø¥Ø¶Ø§ÙØ© Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆÙ…ØµÙÙˆÙØ© Ø§Ù„ØªØºØ·ÙŠØ©ØŒ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø¹Ù‚ÙˆØ¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©ØŒ ÙˆØ§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„.)

---

## 1. Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© (Overview)
Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠ Ù‡Ùˆ **Ø§Ù„Ù‚Ù„Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹**: ÙŠØ³ØªÙ‚Ø¨Ù„ Ø¨ÙŠØªØ§Ù‹ Ø£Ùˆ Ø´Ø·Ø±Ø§Ù‹ Ù…Ù† Ø§Ù„Ø´Ø¹Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ ÙˆÙŠÙØ¬Ø±ÙŠ Ø³Ù„Ø³Ù„Ø© Ù…Ø±Ø§Ø­Ù„ Ù…Ù†Ø¸Ù…Ø© Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠ (Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ØŒ Ø§Ù„Ø¨Ø­Ø±ØŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø¨Ø¯ÙŠÙ„Ø©ØŒ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©ØŒ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø©). ÙŠØ¹ØªÙ…Ø¯ Ù†Ù‡Ø¬Ø§Ù‹ **Ù‚ÙˆØ§Ø¹Ø¯ÙŠØ§Ù‹ Ø£Ø³Ø§Ø³ÙŠØ§Ù‹ ÙÙŠ Ø§Ù„Ù€ MVP** Ù…Ø¹ ØªØµÙ…ÙŠÙ… Ù…Ù‡ÙŠØ£ Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ† (Rules + ML) ÙÙŠ Ù…Ø±Ø­Ù„Ø© Ù„Ø§Ø­Ù‚Ø© Ø¯ÙˆÙ† ÙƒØ³Ø± Ø§Ù„Ø¹Ù‚ÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠØ©.

### 1.1 Ù†Ø·Ø§Ù‚ Ø§Ù„Ù€ MVP (Scope)
Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØºØ·Ø§Ø© ÙÙŠ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©:
1. ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ (Normalization)
2. Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ø£ÙˆÙ„ÙŠ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ (Phonetic / Syllable Segmentation)
3. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠ (Pattern Construction)
4. Ù…Ø·Ø§Ø¨Ù‚Ø© Ø£ÙˆÙ„ÙŠØ© Ù„Ù„Ø¨Ø­ÙˆØ± (Naive Meter Matching)
5. Ø­Ø³Ø§Ø¨ ØªÙ‚Ø¯ÙŠØ±ÙŠ Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØª (Heuristic Quality Score)

Ø§Ù„Ù…Ù‡Ø§Ù… ØºÙŠØ± Ø§Ù„Ù…ÙƒØªÙ…Ù„Ø© (Ù…ÙˆØµÙˆÙØ© Ù‡Ù†Ø§ ÙˆÙ„ÙƒÙ† ØºÙŠØ± Ù…Ù†ÙØ°Ø© Ø¨Ø¹Ø¯ â€“ Ù„Ø§Ø­Ù‚Ø©):
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙØ§Ø¹ÙŠÙ„ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© (Precise TafÄÊ¿Ä«l Decomposition)
- Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø­Ø§ÙØ§Øª ÙˆØ§Ù„Ø¹Ù„Ù„ (Systematic Zihaf/â€˜Ilal Recognition)
- Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø¹ÙˆØ§Ù…Ù„ (Confidence Calibration) â€“ Ø­Ø§Ù„ÙŠØ§Ù‹ Ø«Ù‚Ø© Ø¨Ø¯Ø§Ø¦ÙŠØ©
- Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø¹ØµØ± (Era Detection) â€“ Ù…Ø®Ø·Ø·ØŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯
- Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ† (Adaptive / Ensemble) â€“ Ù…Ø¤Ø¬Ù„
- Ø¢Ù„ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø¬Ø²Ø¦ÙŠ Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„ (Stage-wise Error Recovery)

Ø§Ù†Ø¸Ø± Ø£ÙŠØ¶Ø§Ù‹: `docs/planning/DEFERRED_FEATURES.md` Ù„Ù„Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ù…Ø¤Ø¬Ù„Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù€ MVP.

### 1.2 Ù…Ø¨Ø±Ø±Ø§Øª ØªØ¬Ù…ÙŠØ¯ Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¢Ù†
| Ø§Ù„Ù…ÙŠØ²Ø© | Ø³Ø¨Ø¨ Ø§Ù„ØªØ£Ø¬ÙŠÙ„ | Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ù…Ø³Ø¨Ù‚ | Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø¹Ù†Ø¯ Ø§Ù„ØªÙ‚Ø¯ÙŠÙ… Ù…Ø¨ÙƒØ±Ø§Ù‹ |
|--------|-------------|-----------------|-----------------------------|
| Ø²Ø­Ø§ÙØ§Øª Ø´Ø§Ù…Ù„Ø© | ØªØ­ØªØ§Ø¬ corpus Ù…ÙˆØ³ÙˆÙ… ÙˆØ§Ø³Ø¹ | Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªÙØ§Ø¹ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© | Ù†ØªØ§Ø¦Ø¬ Ù…Ø¶Ù„Ù„Ø©/Ø§Ù†Ø®ÙØ§Ø¶ Ø¯Ù‚Ø© |
| ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§ÙÙŠØ© | Ù‚ÙˆØ§Ø¹Ø¯ ØµØ±ÙÙŠØ©/ØµÙˆØªÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ© | Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ | Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù…Ø¨ÙƒØ±Ø§Ù‹ |
| Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ† ML | Ù†Ù‚Øµ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ ÙƒØ§ÙÙŠØ© | >=1000 Ø¨ÙŠØª Ù…ÙˆØ«ÙˆÙ‚ | ØªØ¹Ù‚ÙŠØ¯ Ù†Ø´Ø± ÙˆØªÙƒØ§Ù„ÙŠÙ |
| Ù…Ø¹Ø§ÙŠØ±Ø© Ø«Ù‚Ø© Ù…ØªÙ‚Ø¯Ù…Ø© | Ø§Ù†ØªØ¸Ø§Ø± Ø¨ÙŠØ§Ù†Ø§Øª Ù‚ÙŠØ§Ø³ | ØªÙˆØ«ÙŠÙ‚ Ù†ÙˆØ§ØªØ¬ Ø­Ø§Ù„ÙŠØ© | Ø«Ù‚Ø© ØºÙŠØ± ÙˆØ§Ù‚Ø¹ÙŠØ© |

### 1.3 Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ·ÙˆØ± (Hybrid Strategy Roadmap)
Ø§Ù„ØªØ­ÙˆÙ„ Ù…Ù† Ù‚ÙˆØ§Ø¹Ø¯ ÙÙ‚Ø· Ø¥Ù„Ù‰ Ù†Ù‡Ø¬ Ù‡Ø¬ÙŠÙ† Ø³ÙŠØªÙ… Ø¹Ù„Ù‰ Ø«Ù„Ø§Ø« Ø·Ø¨Ù‚Ø§Øª:
1. ØªØ­Ø³ÙŠÙ† Ø·Ø¨Ù‚Ø© Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ (Ø´Ø¯Ø©ØŒ ØªÙ†ÙˆÙŠÙ†ØŒ Ù…Ø¯ØŒ Ø³ÙŠØ§Ù‚ Ø§Ù„ÙˆÙ‚ÙØŒ Ø§Ù„ØªÙØ§Ø¹ÙŠÙ„) â€“ Weeks 3â€“8.
2. Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© + ØªÙˆØ³ÙŠØ¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠØ© (Golden / Silver Sets) â€“ Weeks 6â€“10.
3. Ø¥Ø¯Ø®Ø§Ù„ Ù†Ù…ÙˆØ°Ø¬ ML Ù…Ø³Ø§Ø¹Ø¯ (Meter Classifier / Pattern Scorer) â€“ Post-Beta (Phase 2) ÙˆÙÙ‚ Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø¨ÙˆÙ„ (Ø§Ù†Ø¸Ø± Acceptance Gate Ù„Ø§Ø­Ù‚Ø§Ù‹).

---

## 2. Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© (Architecture)

### 2.1 Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ
| Ø§Ù„Ø·Ø¨Ù‚Ø© | Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© | Ø§Ù„Ù…Ø¯Ø®Ù„ | Ø§Ù„Ù…Ø®Ø±Ø¬ | Ø£Ø®Ø·Ø§Ø¡ Ù…Ø­ØªÙ…Ù„Ø© | ÙˆØ¶Ø¹ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø­Ø§Ù„ÙŠ |
|--------|-----------|--------|--------|--------------|---------------------|
| Normalizer | ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø­Ø±ÙˆÙØŒ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)ØŒ ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ±Ù‚ÙŠÙ… | Ù†Øµ Ø®Ø§Ù… | Ù†Øµ Ù…Ù†Ø³Ù‚ | Ø­Ø±ÙˆÙ ØºÙŠØ± Ø¹Ø±Ø¨ÙŠØ©ØŒ Ù†Øµ ÙØ§Ø±Øº | Ù…Ù†Ø¬Ø² (MVP) |
| Preprocessor | ØªØ·Ø¨ÙŠÙ‚ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø´Ø¯Ø©/Ø§Ù„ØªÙ†ÙˆÙŠÙ†/Ø§Ù„Ù…Ø¯ Ù‚Ø¨Ù„ Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ | Ù†Øµ Ù…Ù†Ø³Ù‚ | Ù†Øµ Ù…ÙÙ‡ÙŠØ£ Ù„Ù„ØªÙ‚Ø·ÙŠØ¹ | ÙØ´Ù„ ÙÙŠ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ø´Ø¯Ø© | ØºÙŠØ± Ù…Ù†ÙÙ‘Ø° Ø¨Ø¹Ø¯ |
| Segmenter | ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ ØµÙˆØªÙŠØ© Ø£ÙˆÙ„ÙŠØ© | Ù†Øµ Ù…ÙÙ‡ÙŠØ£ | Ù‚Ø§Ø¦Ù…Ø© Ù…Ù‚Ø§Ø·Ø¹ | ØºÙ…ÙˆØ¶ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ù‚Ø·Ø¹ | Ù…Ù†Ø¬Ø² (Ø¨Ø¯Ø§Ø¦ÙŠ) |
| Pattern Builder | ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² (-/u) ÙˆØªØ¬Ù…ÙŠØ¹Ù‡Ø§ | Ù…Ù‚Ø§Ø·Ø¹ | Ù†Ù…Ø· Ø¹Ø±ÙˆØ¶ÙŠ Ø£ÙˆÙ„ÙŠ | Ø·ÙˆÙ„ ØºÙŠØ± Ù…ØªÙ†Ø§Ø³Ù‚ | Ù…Ù†Ø¬Ø² (Ø¨Ø¯Ø§Ø¦ÙŠ) |
| TafÄÊ¿Ä«l Parser | Ù…Ø·Ø§Ø¨Ù‚Ø© Ù‚Ø·Ø¹ Ø§Ù„Ù†Ù…Ø· Ø¥Ù„Ù‰ ØªÙØ§Ø¹ÙŠÙ„ ÙˆØ²Ø­Ø§ÙØ§Øª | Ù†Ù…Ø· | Ù‚Ø§Ø¦Ù…Ø© ØªÙØ§Ø¹ÙŠÙ„ + Ù…ØªØºÙŠØ±Ø§Øª | Ø¹Ø¯Ù… ØªØ·Ø§Ø¨Ù‚ Ø£Ùˆ ØªÙ‚Ø·ÙŠØ¹ Ø®Ø§Ø·Ø¦ | ØºÙŠØ± Ù…Ù†ÙÙ‘Ø° |
| Meter Matcher | Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¨Ø­Ø± + Ø¨Ø¯Ø§Ø¦Ù„ | ØªÙØ§Ø¹ÙŠÙ„ Ø£Ùˆ Ù†Ù…Ø· | MeterResult | ØªØ¹Ø¯Ø¯ Ø¨Ø­ÙˆØ± Ù…Ø­ØªÙ…Ù„Ø© | Ù…Ù†Ø¬Ø² (Ù…Ø¨Ø³Ù‘Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…Ø· Ù…Ø¨Ø§Ø´Ø±Ø©) |
| Confidence Calibrator | ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø§Ù… Ø­Ø³Ø¨ Ø¹ÙˆØ§Ù…Ù„ | MeterResult Ø®Ø§Ù…ØŒ Ø³ÙŠØ§Ù‚ | MeterResult Ù…Ø¹Ø§ÙŠØ± | Ù†Ù‚Øµ Ø¹ÙˆØ§Ù…Ù„ | ØºÙŠØ± Ù…Ù†ÙÙ‘Ø° |
| Quality Assessor | Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø±ÙƒØ¨Ø© | Ù†ÙˆØ§ØªØ¬ Ø³Ø§Ø¨Ù‚Ø© | QualityScore | ØªØºØ°ÙŠØ© ØºÙŠØ± ÙƒØ§ÙÙŠØ© | Ù…Ù†Ø¬Ø² (Ù…Ø¹Ø§Ø¯Ù„Ø© Ø¨Ø¯Ø§Ø¦ÙŠØ©) |
| Error Recovery | Ù†ØªØ§Ø¦Ø¬ Ø¬Ø²Ø¦ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„ Ø§Ù„Ù…Ø±Ø­Ù„ÙŠ | Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª | PartialResult | ØªØ¬Ø§Ù‡Ù„ ÙØ´Ù„ ØµØ§Ù…Øª | ØºÙŠØ± Ù…Ù†ÙÙ‘Ø° |

### 2.2 Ù…Ø®Ø·Ø· Ù…Ø¹Ù…Ø§Ø±ÙŠ Ù…Ø¨Ø³Ù‘Ø·

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         HybridProsodyAnalyzer              â”‚
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   1. Text Pre-processing            â”‚   â”‚
â”‚  â”‚   - Normalization                    â”‚   â”‚
â”‚  â”‚   - Era Detection (Classical/Modern) â”‚   â”‚
â”‚  â”‚   - Complexity Assessment            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â”‚                        â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚        â”‚  Strategy Selector     â”‚            â”‚
â”‚        â”‚  (Rule vs ML vs Both)  â”‚            â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                    â”‚                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚     â”‚              â”‚              â”‚         â”‚
â”‚     â–¼              â–¼              â–¼         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚Rule â”‚      â”‚ ML  â”‚      â”‚ Ensembleâ”‚     â”‚
â”‚  â”‚Basedâ”‚      â”‚Modelâ”‚      â”‚ Voting  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚     â”‚              â”‚              â”‚         â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                    â–¼                        â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚          â”‚ Confidence      â”‚                â”‚
â”‚          â”‚ Calibration     â”‚                â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                    â”‚                        â”‚
â”‚                    â–¼                        â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚          â”‚  Final Result   â”‚                â”‚
â”‚          â”‚  + Alternatives â”‚                â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Ø§Ù„Ø£Ø³Ø³ Ø§Ù„Ù†Ø¸Ø±ÙŠØ© (Prosody Theory Essentials)

### Ø§Ù„ØªÙØ§Ø¹ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Basic Prosodic Feet):

| Ø§Ù„ØªÙØ¹ÙŠÙ„Ø© | Ø§Ù„Ø±Ù…Ø² | Ø§Ù„Ù†Ù…Ø· | Ø§Ù„Ù…Ø«Ø§Ù„ |
|---------|-------|--------|---------|
| ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’ | - u - | Ø·ÙˆÙŠÙ„Ø© Ù‚ØµÙŠØ±Ø© Ø·ÙˆÙŠÙ„Ø© | **Ù‚ÙÙ€**ÙØ§ **Ù†ÙÙ€**Ø¨Ù’**ÙƒÙ** |
| Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙÙ†Ù’ | - - u - | Ø·ÙˆÙŠÙ„Ø© Ø·ÙˆÙŠÙ„Ø© Ù‚ØµÙŠØ±Ø© Ø·ÙˆÙŠÙ„Ø© | **Ù…ÙÙ†Ù’** **Ø°ÙÙƒÙ’**Ø±Ù‰ **Ø­ÙÙ€**Ø¨ÙÙŠØ¨ |
| ÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’ | - u - | Ø·ÙˆÙŠÙ„Ø© Ù‚ØµÙŠØ±Ø© Ø·ÙˆÙŠÙ„Ø© | **Ø¨ÙÙ€**Ø§Ù„Ù’**Ø¹Ù**Ø±ÙØ§Ù‚ |
| Ù…ÙÙÙ’ØªÙØ¹ÙÙ„ÙÙ†Ù’ | - - u - | Ø·ÙˆÙŠÙ„Ø© Ø·ÙˆÙŠÙ„Ø© Ù‚ØµÙŠØ±Ø© Ø·ÙˆÙŠÙ„Ø© | **ÙÙÙŠ** **Ø´ÙÙ€**Ø·ÙÙ‘ **Ø¬ÙÙ€**Ù…ÙÙ‘Ø§Ù† |
| Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ | - - u - | Ø·ÙˆÙŠÙ„Ø© Ø·ÙˆÙŠÙ„Ø© Ù‚ØµÙŠØ±Ø© Ø·ÙˆÙŠÙ„Ø© | **Ø£Ù**Ù„Ø§ **ÙÙÙŠ** **Ø³ÙÙ€**Ø¨ÙÙŠÙ„ |
| ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’ | - u - u - | Ø·ÙˆÙŠÙ„Ø© Ù‚ØµÙŠØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ù‚ØµÙŠØ±Ø© Ø·ÙˆÙŠÙ„Ø© | **ÙŠÙØ§** **Ù„ÙÙ€**ÙŠÙ’**Ù„ÙÙ€**Ø©Ù **Ø§Ù„Ù’Ù€**ØµÙÙ‘Ø¨Ù’Ø± |
| Ù…ÙÙÙØ§Ø¹ÙÙŠÙ„ÙÙ†Ù’ | - u u - | Ø·ÙˆÙŠÙ„Ø© Ù‚ØµÙŠØ±Ø© Ù‚ØµÙŠØ±Ø© Ø·ÙˆÙŠÙ„Ø© | **Ø¨ÙÙ€**Ø§Ù†Ù **Ø§Ù„Ù’Ù€**Ø®Ù**Ù„Ù’Ù€**Ø¯ |
| ÙÙØ¹ÙÙ„ÙØ§ØªÙÙ†Ù’ | - u - u - | Ø·ÙˆÙŠÙ„Ø© Ù‚ØµÙŠØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ù‚ØµÙŠØ±Ø© Ø·ÙˆÙŠÙ„Ø© | **ÙÙÙŠ** **Ø±ÙØ¨ÙÙ€**ÙˆØ¹ **Ø§Ù„Ù€**Ø´ÙÙ‘Ø§Ù… |

### 3.2 Ø§Ù„Ø¨Ø­ÙˆØ± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Canonical Meters)

```
1. Ø§Ù„Ø·ÙˆÙŠÙ„: ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’ Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙÙ†Ù’ ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’ Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙÙ†Ù’
2. Ø§Ù„Ù…Ø¯ÙŠØ¯: ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’ ÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’ ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’ ÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’  
3. Ø§Ù„Ø¨Ø³ÙŠØ·: Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ ÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’ Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ ÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’
4. Ø§Ù„ÙˆØ§ÙØ±: Ù…ÙÙÙØ§Ø¹ÙÙ„ÙØªÙÙ†Ù’ Ù…ÙÙÙØ§Ø¹ÙÙ„ÙØªÙÙ†Ù’ Ù…ÙÙÙØ§Ø¹ÙÙ„ÙØªÙÙ†Ù’
5. Ø§Ù„ÙƒØ§Ù…Ù„: Ù…ÙØªÙÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’ Ù…ÙØªÙÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’ Ù…ÙØªÙÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’
6. Ø§Ù„Ù‡Ø²Ø¬: Ù…ÙÙÙØ§Ø¹ÙÙŠÙ„ÙÙ†Ù’ Ù…ÙÙÙØ§Ø¹ÙÙŠÙ„ÙÙ†Ù’ Ù…ÙÙÙØ§Ø¹ÙÙŠÙ„ÙÙ†Ù’
7. Ø§Ù„Ø±Ø¬Ø²: Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’
8. Ø§Ù„Ø±Ù…Ù„: ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’ ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’ ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’
9. Ø§Ù„Ø³Ø±ÙŠØ¹: Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙØ§ØªÙ
10. Ø§Ù„Ù…Ù†Ø³Ø±Ø­: Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙØ§ØªÙ Ù…ÙÙÙ’ØªÙØ¹ÙÙ„ÙÙ†Ù’
11. Ø§Ù„Ø®ÙÙŠÙ: ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’ Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’
12. Ø§Ù„Ù…Ø¶Ø§Ø±Ø¹: Ù…ÙÙÙØ§Ø¹ÙÙŠÙ„ÙÙ†Ù’ ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’ Ù…ÙÙÙØ§Ø¹ÙÙŠÙ„ÙÙ†Ù’
13. Ø§Ù„Ù…Ù‚ØªØ¶Ø¨: Ù…ÙÙÙ’ØªÙØ¹ÙÙ„ÙÙ†Ù’ Ù…ÙÙÙ’ØªÙØ¹ÙÙ„ÙÙ†Ù’ Ù…ÙÙÙ’ØªÙØ¹ÙÙ„ÙÙ†Ù’
14. Ø§Ù„Ù…Ø¬ØªØ«: Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’ Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’
15. Ø§Ù„Ù…ØªÙ‚Ø§Ø±Ø¨: ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’ ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’ ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’ ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’
16. Ø§Ù„Ù…Ø­Ø¯Ø«: Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙØ§ØªÙ Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙØ§ØªÙ
```

---

## 4. Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© (Algorithmic Stages)
ÙŠÙØ¹Ø±Ø¶ Ø£Ø¯Ù†Ø§Ù‡ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø­Ø§Ù„ÙŠ (Ø§Ù„Ù…Ù†Ø¬Ø²) Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¹ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø­Ø§Ù„Ø© ÙƒÙ„ Ø¬Ø²Ø¡.

### 4.1 Ø§Ù„ØªØ·Ø¨ÙŠØ¹ (Normalization) â€“ Ù…Ù†Ø¬Ø²

### 1ï¸âƒ£ ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ (Text Normalization)

```python
class ArabicNormalizer:
    """
    ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠ
    """
    
    def __init__(self):
        # Ø®Ø±ÙŠØ·Ø© ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
        self.char_map = {
            'Ø£': 'Ø§', 'Ø¥': 'Ø§', 'Ø¢': 'Ø§',  # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ù„Ù
            'Ø©': 'Ù‡',                        # ØªØ§Ø¡ Ù…Ø±Ø¨ÙˆØ·Ø© Ø¥Ù„Ù‰ Ù‡Ø§Ø¡
            'ÙŠ': 'Ù‰',                       # ÙŠØ§Ø¡ Ø¥Ù„Ù‰ Ø£Ù„Ù Ù…Ù‚ØµÙˆØ±Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        }
        
        # Ø£Ø­Ø±Ù Ø§Ù„ØªØ´ÙƒÙŠÙ„
        self.diacritics = 'Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù°Ù±Ù²Ù³Ù´ÙµÙ¶Ù·Ù¸Ù¹ÙºÙ»Ù¼Ù½Ù¾Ù¿'
        
    def normalize(self, text: str) -> str:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        normalized = self.remove_diacritics(text)
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
        for old_char, new_char in self.char_map.items():
            normalized = normalized.replace(old_char, new_char)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙØ±Ø§ØºØ§Øª ÙˆØ§Ù„ØªØ±Ù‚ÙŠÙ…
        normalized = self.clean_punctuation(normalized)
        
        return normalized.strip()
    
    def remove_diacritics(self, text: str) -> str:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„"""
        return ''.join(c for c in text if c not in self.diacritics)
    
    def clean_punctuation(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ±Ù‚ÙŠÙ… ÙˆØ§Ù„ÙØ±Ø§ØºØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©"""
        import re
        
        # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…
        text = re.sub(r'[ØŒØ›ØŸ!()."\'-]', ' ', text)
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„ÙØ±Ø§ØºØ§Øª
        text = re.sub(r'\s+', ' ', text)
        
        return text
```

### 4.2 Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©/Ø§Ù„ØµÙˆØªÙŠØ© (Preprocessing Rules) â€“ Ù…Ø®Ø·Ø· (Pending)

Ù„Ø¨Ù†Ø§Ø¡ Ù…Ø­Ù„Ù„ Ù…ÙˆØ«ÙˆÙ‚ØŒ Ù†ÙØ¹Ø±Ù‘Ù Ù‚ÙˆØ§Ø¹Ø¯ ÙˆØ§Ø¶Ø­Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±. ØªÙØ·Ø¨Ù‚ Ø¨Ù‡Ø°Ø§ Ø§Ù„ØªØ±ØªÙŠØ¨ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§ØªØ³Ø§Ù‚:

1) Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯
- Rule-0: ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ±Ù‚ÙŠÙ… ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª
- Rule-1: ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ù„ÙØ§Øª ÙˆØ§Ù„Ù‡Ù…Ø²Ø§Øª (Ø£/Ø¥/Ø¢ â†’ Ø§)ØŒ ÙˆØ§Ù„Ù‡Ù…Ø²Ø© Ø§Ù„Ù…ØªØ·Ø±ÙØ© Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚
- Rule-2: ÙÙƒ Ø§Ù„Ø´Ù‘ÙØ¯Ù‘Ø© (Ø´Ø¯Ø©) Ø¥Ù„Ù‰ Ø­Ø±ÙÙŠÙ† Ù…ØªÙ…Ø§Ø«Ù„ÙŠÙ† Ù…Ø¹ Ø³ÙƒÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆÙ„ ÙˆØ­Ø±ÙƒØ© Ø¹Ù„Ù‰ Ø§Ù„Ø«Ø§Ù†ÙŠ
- Rule-3: Ø§Ù„ÙˆÙ‚Ù Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†ÙˆÙŠÙ†: Ø¹Ù†Ø¯ Ø§Ù„ÙˆÙ‚Ù ØªÙØ¨Ø¯Ù‘Ù„ ØªÙ†ÙˆÙŠÙ† Ø§Ù„Ù†ØµØ¨ Ø¥Ù„Ù‰ Ø£Ù„Ù Ø¥Ø´Ø¨Ø§Ø¹ Â«Ø§Ù‹ â†’ Ø§Â»ØŒ ÙˆØªÙ†ÙˆÙŠÙ† Ø§Ù„Ø¶Ù…/Ø§Ù„ÙƒØ³Ø± ÙŠÙØ­Ø°Ù Ù…Ø¹ Ø¥Ø³ÙƒØ§Ù† Ø§Ù„Ø­Ø±Ù (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ø­Ø³Ø¨ Ù†Ù…Ø· Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„)
- Rule-4: Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¯Ù‘ (Ø§ØŒ ÙˆØŒ ÙŠ) ØªØ¹Ø§Ù…Ù„ ÙƒÙ€ Â«Ø­Ø±ÙƒØ© Ø·ÙˆÙŠÙ„Ø©Â» Ø¹Ù†Ø¯Ù…Ø§ ØªØ³Ø¨Ù‚Ù‡Ø§ Ø­Ø±ÙƒØ© Ù…Ù† Ø¬Ù†Ø³Ù‡Ø§
- Rule-5: Ø§Ù„Ø³ÙƒÙˆÙ† Ø§Ù„Ù…ØªØªØ§Ø¨Ø¹: Ù…Ù†Ø¹ ØªÙˆÙ„ÙŠØ¯ Ø¹Ù†Ø§Ù‚ÙŠØ¯ Ø³Ø§ÙƒÙ†Ø© Ù…Ø®Ø§Ù„ÙØ© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ©ØŒ Ù…Ø¹ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ù…Ø§ ÙŠØ¬ÙŠØ²Ù‡ Ø§Ù„Ø¹Ø±ÙˆØ¶ (Ù…Ø«Ù„ Ø§Ù„ØªÙ‚Ø§Ø¡ Ø³Ø§ÙƒÙ†ÙŠÙ† Ø¹Ø¨Ø± Ø§Ù„ÙˆÙ‚Ù)

2) ÙÙƒÙ‘ Ø§Ù„Ø´Ø¯Ø© (Shadda Decomposition)
- Ù…ÙØ¯Ø®Ù„: Â«Ø§Ù„Ø´Ù‘ÙØ¹Ø±Â»  â†’ Ù…ÙØ®Ø±ÙØ¬: Â«Ø§Ù„Ø´Ù’Ø´Ø¹Ø±Â» Ù‚Ø¨Ù„ Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ (Ø­Ø±ÙØ§Ù†: Ø§Ù„Ø£ÙˆÙ„ Ø³Ø§ÙƒÙ†ØŒ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù…ØªØ­Ø±Ùƒ)
- Ø§Ù„ØªØ£Ø«ÙŠØ± Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠ: ØºØ§Ù„Ø¨Ø§Ù‹ ÙŠØ­ÙˆÙ‘Ù„ CV Ø¥Ù„Ù‰ CVC Ø£Ùˆ ÙŠÙØ·ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚

3) Ø§Ù„ÙˆÙ‚Ù ÙˆØ§Ù„ØªÙ†ÙˆÙŠÙ† (Pause/TanwÄ«n)
- Ø§Ù„ÙˆÙ‚Ù Ø¹Ù„Ù‰ Â«Ø§Ù‹Â»: ØªØªØ­ÙˆÙ„ Ø¥Ù„Ù‰ Â«Ø§Â» Ø·ÙˆÙŠÙ„Ø© (Ø¥Ø´Ø¨Ø§Ø¹ Ù…Ø¯Ù‘ÙŠ)
- Ø§Ù„ÙˆÙ‚Ù Ø¹Ù„Ù‰ Â«ÙŒ/ÙÂ»: ØªÙØ­Ø°Ù Ø§Ù„Ø­Ø±ÙƒØ© ÙˆÙŠÙØ³ÙƒÙ† Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø£Ø®ÙŠØ±: Â«Ø¹ÙÙ„Ù’Ù…ÙŒÂ» â†’ Â«Ø¹ÙÙ„Ù’Ù…Ù’Â»
- ÙÙŠ Ø­Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„: Ù†ÙØªØ±Ø¶ Ø§Ù„ÙˆÙ‚Ù Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ Ø¹Ù†Ø¯ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø´Ø·Ø±

4) Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¯Ù‘ (Madd Letters)
- Ø§Ù„Ø£Ù„Ù Ø¨Ø¹Ø¯ ÙØªØ­Ø©: Â«ÙØ§Â» = CVV
- Ø§Ù„ÙˆØ§Ùˆ Ø¨Ø¹Ø¯ Ø¶Ù…Ø©: Â«ÙÙˆÂ» = CVV
- Ø§Ù„ÙŠØ§Ø¡ Ø¨Ø¹Ø¯ ÙƒØ³Ø±Ø©: Â«ÙÙŠÂ» = CVV
- Ø¥Ù† Ù„Ù… ÙŠØ³Ø¨Ù‚Ù‡Ø§ Ø­Ø±ÙƒØ© Ù…Ø¬Ø§Ù†Ø³Ø©ØŒ ØªÙØ¹Ø§Ù…Ù„ ÙƒØµØ§Ù…Øª ÙˆÙÙ‚ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠ (Ø®Ø§ØµØ© Ø§Ù„ÙˆØ§Ùˆ/Ø§Ù„ÙŠØ§Ø¡ Ø§Ù„Ù„ÙŠÙ†ØªØ§Ù†)

5) Ø¹Ù†Ø§Ù‚ÙŠØ¯ Ø§Ù„Ø³ÙƒÙˆÙ† (SukÅ«n Clusters)
- Ø¥Ø°Ø§ Ù†ØªØ¬ C + S + C Ø¨Ø¯Ø§ÙŠØ© Ù…Ù‚Ø·Ø¹ØŒ Ù†Ø·Ø¨Ù‚ Ù‚Ø§Ø¹Ø¯Ø© ÙƒØ³Ø± Ø§Ù„Ø¹Ù†Ø§Ù‚ÙŠØ¯: Ø¥Ø³Ù†Ø§Ø¯ Ø§Ù„Ø³ÙƒÙˆÙ† Ù„Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„Ø³Ø§Ø¨Ù‚ Ø£Ùˆ Ø¥Ø¯Ø±Ø§Ø¬ Ù…Ø¯Ù‘ Ø¥Ø´Ø¨Ø§Ø¹ÙŠ Ø¹Ù†Ø¯ Ø§Ù„ÙˆÙ‚Ù ÙˆÙÙ‚ Ø¹Ø±Ù Ø§Ù„Ø¹Ø±ÙˆØ¶

Ù…Ù„Ø§Ø­Ø¸Ø©: Ù†ÙÙØ¹Ù‘Ù„/Ù†ÙØ¹Ø·Ù‘Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø¹Ø¨Ø± Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø¯Ù‚Ø©.

### âœ… Ø¬Ø¯Ø§ÙˆÙ„ Ù…Ø±Ø¬Ø¹ÙŠØ© Ø³Ø±ÙŠØ¹Ø©

| Ø§Ù„Ø¸Ø§Ù‡Ø±Ø© | Ù‚Ø¨Ù„ | Ø¨Ø¹Ø¯ (Ù„Ù„ØªÙ‚Ø·ÙŠØ¹) | Ù…Ù„Ø§Ø­Ø¸Ø© |
|---|---|---|---|
| Ø§Ù„Ø´Ø¯Ø© | Ù…ÙØ¯Ù‘Ù | Ù…ÙØ¯Ù’Ø¯Ù | Ø­Ø±ÙØ§Ù†: Ø§Ù„Ø£ÙˆÙ„ Ø³Ø§ÙƒÙ† |
| ØªÙ†ÙˆÙŠÙ† Ø§Ù„Ù†ØµØ¨ ÙˆÙ‚ÙØ§ | Ø±ÙØ¬ÙÙ„Ø§Ù‹# | Ø±ÙØ¬ÙÙ„Ø§ | Ù…Ø¯Ù‘ Ø¥Ø´Ø¨Ø§Ø¹ |
| ØªÙ†ÙˆÙŠÙ† Ø§Ù„Ø¶Ù…/Ø§Ù„ÙƒØ³Ø± ÙˆÙ‚ÙØ§ | Ø¹ÙÙ„Ù’Ù…ÙŒ# | Ø¹ÙÙ„Ù’Ù…Ù’ | Ø¥Ø³ÙƒØ§Ù† Ù†Ù‡Ø§Ø¦ÙŠ |
| Ù…Ø¯Ù‘ ÙØªØ­Ø© + Ø£Ù„Ù | ÙÙØ§ | ÙØ§ | CVV |
| Ù…Ø¯Ù‘ Ø¶Ù…Ø© + ÙˆØ§Ùˆ | ÙÙÙˆ | ÙÙˆ | CVV |
| Ù…Ø¯Ù‘ ÙƒØ³Ø±Ø© + ÙŠØ§Ø¡ | ÙÙÙŠ | ÙÙŠ | CVV |

ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ ÙŠØ³Ø¨Ù‚ Ø¯Ø§Ù„Ø© `segment()` ÙˆÙŠÙØ´Ø±Ù Ø¹Ù„ÙŠÙ‡ Ø§Ù„Ù€ Normalizer/Preprocessor.

### 4.3 Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ØµÙˆØªÙŠ (Phonetic Segmentation) â€“ Ù…Ù†Ø¬Ø² (Ø¨Ø¯Ø§Ø¦ÙŠ)

```python
class PhonemicSegmenter:
    """
    ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ ØµÙˆØªÙŠØ© (syllables)
    """
    
    def __init__(self):
        self.vowels = 'Ø§ÙˆÙŠØ§Ø¤Ø¦Ø©'
        self.consonants = 'Ø¨ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙŠ'
        
    def segment(self, text: str) -> List[Syllable]:
        """ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ ØµÙˆØªÙŠØ©"""
        syllables = []
        current_syllable = ''
        
        i = 0
        while i < len(text):
            char = text[i]
            
            if char == ' ':
                if current_syllable:
                    syllables.append(self.analyze_syllable(current_syllable))
                    current_syllable = ''
                i += 1
                continue
            
            current_syllable += char
            
            # Ù‚ÙˆØ§Ø¹Ø¯ ØªØ­Ø¯ÙŠØ¯ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ù‚Ø·Ø¹
            if self.is_syllable_complete(current_syllable, text[i+1:] if i+1 < len(text) else ''):
                syllables.append(self.analyze_syllable(current_syllable))
                current_syllable = ''
            
            i += 1
        
        if current_syllable:
            syllables.append(self.analyze_syllable(current_syllable))
        
        return syllables
    
    def is_syllable_complete(self, current: str, remaining: str) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù‚Ø·Ø¹ Ù…ÙƒØªÙ…Ù„Ø§Ù‹"""
        # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠ
        
        # Ù…Ù‚Ø·Ø¹ Ù‚ØµÙŠØ±: ØµØ§Ù…Øª + Ø­Ø±ÙƒØ© Ù‚ØµÙŠØ±Ø©
        if len(current) == 2 and current[0] in self.consonants and current[1] in 'Ø§ÙˆÙŠ':
            return True
            
        # Ù…Ù‚Ø·Ø¹ Ø·ÙˆÙŠÙ„: ØµØ§Ù…Øª + Ø­Ø±ÙƒØ© Ø·ÙˆÙŠÙ„Ø©
        if len(current) == 2 and current[0] in self.consonants and current[1] in self.vowels:
            return True
        
        # Ù…Ù‚Ø·Ø¹ Ù…Ø®ØªÙ„Ø·: ØµØ§Ù…Øª + Ø­Ø±ÙƒØ© + ØµØ§Ù…Øª
        if len(current) == 3:
            return True
        
        return False
    
    def analyze_syllable(self, syllable: str) -> 'Syllable':
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø·Ø¹ ÙˆØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹Ù‡"""
        return Syllable(
            text=syllable,
            type=self.determine_syllable_type(syllable),
            length=self.determine_length(syllable),
            stress=self.determine_stress(syllable)
        )

@dataclass
class Syllable:
    text: str
    type: str  # 'CV', 'CVV', 'CVC', etc.
    length: str  # 'short', 'long'
    stress: bool  # True if stressed
```

### 4.4 ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø· ÙˆØ§Ù„ØªÙØ§Ø¹ÙŠÙ„ (Pattern & TafÄÊ¿Ä«l) â€“ Ù…Ø®Ø·Ø·
Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø­Ø§Ù„ÙŠ ÙŠØªØ¬Ø§ÙˆØ² Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªÙØ§Ø¹ÙŠÙ„ ÙˆÙŠØ°Ù‡Ø¨ Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¥Ù„Ù‰ Ù†Ù…Ø· Ø±Ù…Ø²ÙŠ Ø«Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¨Ø­Ø±.
Ø§Ù„Ø®Ø·Ø©:
1. Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ ØªÙØ§Ø¹ÙŠÙ„ Ø£Ø³Ø§Ø³ÙŠ (8 ØªÙØ§Ø¹ÙŠÙ„) Ù…Ø¹ Ø£Ù†Ù…Ø§Ø· Ø´ÙƒÙ„ÙŠØ©.
2. ØªØ·Ø¨ÙŠÙ‚ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø£Ø·ÙˆÙ„ ØªØ·Ø§Ø¨Ù‚ (Longest Match) Ù…Ø¹ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø²Ø­Ø§ÙØ§Øª Ø´Ø§Ø¦Ø¹Ø©.
3. ØªØ®Ø²ÙŠÙ† ÙƒÙ„ ØªÙØ¹ÙŠÙ„Ø© ÙÙŠ ÙƒØ§Ø¦Ù† ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ø§Ø³Ù…ØŒ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø£ØµÙ„ÙŠØŒ Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
4. ØªÙ…Ø±ÙŠØ± Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªÙØ§Ø¹ÙŠÙ„ Ø¥Ù„Ù‰ Ù…ÙƒØªØ´Ù Ø§Ù„Ø¨Ø­Ø± Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø®Ø§Ù….

```python
class ProsodyAnalyzer:
    """
    ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠØ©
    """
    
    def __init__(self):
        self.load_prosodic_patterns()
    
    def analyze_rhythm(self, syllables: List[Syllable]) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¥Ù„Ù‰ Ù†Ù…Ø· Ø¹Ø±ÙˆØ¶ÙŠ"""
        pattern = ''
        
        for syllable in syllables:
            if syllable.length == 'short':
                pattern += 'u'  # Ù‚ØµÙŠØ± (unstressed)
            else:
                pattern += '-'  # Ø·ÙˆÙŠÙ„ (stressed)
        
        return pattern
    
    def identify_tafa3il(self, pattern: str) -> List[Taf3ila]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªÙØ§Ø¹ÙŠÙ„ ÙÙŠ Ø§Ù„Ù†Ù…Ø·"""
        tafa3il = []
        
        # Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ØªÙØ§Ø¹ÙŠÙ„ - Ø§Ù†Ø¸Ø± ZIHAFAT_LOOKUP_TABLE Ø£Ø¯Ù†Ø§Ù‡
        taf3ila_patterns = self._load_taf3ila_patterns()
        
        i = 0
        while i < len(pattern):
            found_match = False
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø·ÙˆÙ„ ØªØ·Ø§Ø¨Ù‚
            for length in range(5, 2, -1):  # Ù…Ù† 5 Ø¥Ù„Ù‰ 3 Ø£Ø­Ø±Ù
                if i + length <= len(pattern):
                    substr = pattern[i:i+length]
                    if substr in taf3ila_patterns:
                        tafa3il.append(Taf3ila(
                            name=taf3ila_patterns[substr],
                            pattern=substr,
                            position=len(tafa3il) + 1
                        ))
                        i += length
                        found_match = True
                        break
            
            if not found_match:
                i += 1
        
        return tafa3il

@dataclass 
class Taf3ila:
    name: str
    pattern: str
    position: int
    variations: List[str] = field(default_factory=list)

---

### 4.5 Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ø­Ø§ÙØ§Øª ÙˆØ§Ù„Ø¹Ù„Ù„ (Zihafat Lookup) â€“ Ù…Ø®Ø·Ø·

### Purpose
This lookup table contains all valid variations (Ø²Ø­Ø§ÙØ§Øª ÙˆØ¹Ù„Ù„) for the 8 basic prosodic feet (ØªÙØ§Ø¹ÙŠÙ„). Use this during pattern matching to recognize verses with Ø²Ø­Ø§ÙØ§Øª.

**Reference:** Based on Ø§Ù„Ø®Ù„ÙŠÙ„ Ø¨Ù† Ø£Ø­Ù…Ø¯ Ø§Ù„ÙØ±Ø§Ù‡ÙŠØ¯ÙŠ's ÙƒØªØ§Ø¨ Ø§Ù„Ø¹Ø±ÙˆØ¶

```python
# app/core/prosody/zihafat.py

from typing import Dict, List
from dataclasses import dataclass

@dataclass
class ZihafVariation:
    """Represents a zihaf variation of a taf'ila"""
    name: str  # Ø§Ø³Ù… Ø§Ù„Ø²Ø­Ø§Ù
    pattern: str  # Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¥ÙŠÙ‚Ø§Ø¹ÙŠ
    frequency: str  # "common", "rare", "very_rare"
    description_ar: str
    example: str = ""

# =====================================================
# Ø§Ù„ØªÙØ§Ø¹ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø«Ù…Ø§Ù†ÙŠØ© Ù…Ø¹ Ø²Ø­Ø§ÙØ§ØªÙ‡Ø§
# =====================================================

ZIHAFAT_LOOKUP: Dict[str, List[ZihafVariation]] = {
    
    # =========================================
    # 1. ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’ (fa'oolun)
    # =========================================
    "ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’": [
        ZihafVariation(
            name="Ø§Ù„Ø£ØµÙ„ (ØµØ­ÙŠØ­)",
            pattern="- u - -",  # Ø·ÙˆÙŠÙ„ Ù‚ØµÙŠØ± Ø·ÙˆÙŠÙ„ Ø·ÙˆÙŠÙ„
            frequency="common",
            description_ar="Ø§Ù„ØªÙØ¹ÙŠÙ„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø²Ø­Ø§Ù",
            example="Ù‚ÙÙØ§ Ù†ÙØ¨Ù’ÙƒÙ (ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’)"
        ),
        ZihafVariation(
            name="ÙÙØ¹ÙÙˆÙ„Ù (Ø§Ù„Ù‚Ø¨Ø¶)",
            pattern="- u - u",  # Ø­Ø°Ù Ø§Ù„Ø®Ø§Ù…Ø³ Ø§Ù„Ø³Ø§ÙƒÙ†
            frequency="very_common",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø®Ø§Ù…Ø³ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„Ù†ÙˆÙ†)",
            example="Ù…ÙÙ†Ù’ Ø°ÙÙƒÙ’Ø±Ù‰ (ÙÙØ¹ÙÙˆÙ„Ù)"
        ),
        ZihafVariation(
            name="ÙÙØ¹ÙÙˆ (Ø§Ù„Ø­Ø°Ù)",
            pattern="- u -",  # Ø­Ø°Ù Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø®ÙÙŠÙ Ù…Ù† Ø¢Ø®Ø±Ù‡
            frequency="rare",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø®ÙÙŠÙ Ù…Ù† Ø¢Ø®Ø±Ù‡ (Ù„Ù†)",
            example="Ù†Ø§Ø¯Ø± ÙÙŠ Ø§Ù„Ø·ÙˆÙŠÙ„"
        ),
        ZihafVariation(
            name="ÙÙØ¹Ù’Ù„ÙÙ†Ù’ (Ø§Ù„Ø·ÙŠ)",
            pattern="- - - -",  # Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø±Ø§Ø¨Ø¹ Ø§Ù„Ø³Ø§ÙƒÙ†
            frequency="rare",
            description_ar="Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø±Ø§Ø¨Ø¹ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„ÙˆØ§Ùˆ)",
            example="Ù†Ø§Ø¯Ø± Ø¬Ø¯Ø§Ù‹"
        ),
    ],
    
    # =========================================
    # 2. Ù…ÙÙÙØ§Ø¹ÙÙŠÙ„ÙÙ†Ù’ (mafa'eelun)
    # =========================================
    "Ù…ÙÙÙØ§Ø¹ÙÙŠÙ„ÙÙ†Ù’": [
        ZihafVariation(
            name="Ø§Ù„Ø£ØµÙ„ (ØµØ­ÙŠØ­)",
            pattern="- u u - -",  # Ø·ÙˆÙŠÙ„ Ù‚ØµÙŠØ± Ù‚ØµÙŠØ± Ø·ÙˆÙŠÙ„ Ø·ÙˆÙŠÙ„
            frequency="common",
            description_ar="Ø§Ù„ØªÙØ¹ÙŠÙ„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©",
            example="Ø¨ÙØ§Ù†Ù Ø§Ù„Ø®ÙÙ„Ù’Ø¯Ù (Ù…ÙÙÙØ§Ø¹ÙÙŠÙ„ÙÙ†Ù’)"
        ),
        ZihafVariation(
            name="Ù…ÙÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’ (Ø§Ù„Ù‚Ø¨Ø¶)",
            pattern="- u - - -",  # ØªØ³ÙƒÙŠÙ† Ø§Ù„Ø®Ø§Ù…Ø³ Ø§Ù„Ù…ØªØ­Ø±Ùƒ
            frequency="very_common",
            description_ar="ØªØ³ÙƒÙŠÙ† Ø§Ù„Ø®Ø§Ù…Ø³ Ø§Ù„Ù…ØªØ­Ø±Ùƒ (Ø§Ù„ÙŠØ§Ø¡ â†’ ÙŠÙ’)",
            example="Ø´Ø§Ø¦Ø¹ ÙÙŠ Ø§Ù„Ù‡Ø²Ø¬"
        ),
        ZihafVariation(
            name="Ù…ÙÙÙØ§Ø¹ÙÙŠ (Ø§Ù„ÙƒØ³Ù)",
            pattern="- u u -",  # Ø­Ø°Ù Ø§Ù„Ø³Ø§Ø¨Ø¹ Ø§Ù„Ø³Ø§ÙƒÙ†
            frequency="rare",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø³Ø§Ø¨Ø¹ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„Ù†ÙˆÙ†)",
            example="Ù†Ø§Ø¯Ø±"
        ),
    ],
    
    # =========================================
    # 3. Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ (mustaf'ilun)
    # =========================================
    "Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’": [
        ZihafVariation(
            name="Ø§Ù„Ø£ØµÙ„ (ØµØ­ÙŠØ­)",
            pattern="- - u - -",  # Ø·ÙˆÙŠÙ„ Ø·ÙˆÙŠÙ„ Ù‚ØµÙŠØ± Ø·ÙˆÙŠÙ„ Ø·ÙˆÙŠÙ„
            frequency="common",
            description_ar="Ø§Ù„ØªÙØ¹ÙŠÙ„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©",
            example="Ø£ÙÙ„Ø§ ÙÙÙŠ Ø³ÙØ¨ÙŠÙ„Ù (Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’)"
        ),
        ZihafVariation(
            name="Ù…ÙØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ (Ø§Ù„Ø®Ø¨Ù†)",
            pattern="u - u - -",  # Ø­Ø°Ù Ø§Ù„Ø«Ø§Ù†ÙŠ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„Ø³ÙŠÙ†)
            frequency="very_common",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø«Ø§Ù†ÙŠ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„Ø³ÙŠÙ†)",
            example="Ø§Ù„Ù’Ù…ÙØ¬Ù’Ø¯Ù Ù…Ø§ (Ù…ÙØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’) - Ø´Ø§Ø¦Ø¹ Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ù„Ø±Ø¬Ø²"
        ),
        ZihafVariation(
            name="Ù…ÙØ³Ù’ØªÙØ¹ÙÙ„ÙÙ†Ù’ (Ø§Ù„Ø·ÙŠ)",
            pattern="- - - - -",  # Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø±Ø§Ø¨Ø¹ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„ÙØ§Ø¡)
            frequency="common",
            description_ar="Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø±Ø§Ø¨Ø¹ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„ÙØ§Ø¡)",
            example="Ø´Ø§Ø¦Ø¹ ÙÙŠ Ø§Ù„Ø¨Ø³ÙŠØ·"
        ),
        ZihafVariation(
            name="Ù…ÙØªÙØ¹ÙÙ„ÙÙ†Ù’ (Ø§Ù„Ø®Ø¨Ù† + Ø§Ù„Ø·ÙŠ)",
            pattern="u - - - -",  # Ø®Ø¨Ù† ÙˆØ·ÙŠ Ù…Ø¹Ø§Ù‹
            frequency="rare",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø³ÙŠÙ† ÙˆØ§Ù„ÙØ§Ø¡ Ù…Ø¹Ø§Ù‹",
            example="Ù†Ø§Ø¯Ø± - Ø²Ø­Ø§Ù Ù…Ø±ÙƒØ¨"
        ),
        ZihafVariation(
            name="Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„Ù (Ø§Ù„Ù‚Ø¨Ø¶)",
            pattern="- - u - u",  # Ø­Ø°Ù Ø§Ù„Ø®Ø§Ù…Ø³ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„Ù†ÙˆÙ†)
            frequency="rare",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø®Ø§Ù…Ø³ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„Ù†ÙˆÙ†)",
            example="Ù†Ø§Ø¯Ø± ÙÙŠ Ø§Ù„Ø±Ø¬Ø²"
        ),
    ],
    
    # =========================================
    # 4. ÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’ (fa'ilun)
    # =========================================
    "ÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’": [
        ZihafVariation(
            name="Ø§Ù„Ø£ØµÙ„ (ØµØ­ÙŠØ­)",
            pattern="- u - -",  # Ø·ÙˆÙŠÙ„ Ù‚ØµÙŠØ± Ø·ÙˆÙŠÙ„ Ø·ÙˆÙŠÙ„
            frequency="common",
            description_ar="Ø§Ù„ØªÙØ¹ÙŠÙ„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©",
            example="Ø¨ÙØ§Ù„Ø¹ÙØ±Ø§Ù‚Ù (ÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’)"
        ),
        ZihafVariation(
            name="ÙÙØ¹ÙÙ„ÙÙ†Ù’ (Ø§Ù„Ø®Ø¨Ù†)",
            pattern="- - - -",  # Ø­Ø°Ù Ø§Ù„Ø«Ø§Ù†ÙŠ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„Ø£Ù„Ù)
            frequency="very_common",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø«Ø§Ù†ÙŠ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„Ø£Ù„Ù)",
            example="Ø´Ø§Ø¦Ø¹ Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ù„Ø±Ù…Ù„ ÙˆØ§Ù„Ø®ÙÙŠÙ"
        ),
        ZihafVariation(
            name="ÙÙØ§Ø¹ÙÙ„Ù (Ø§Ù„Ù‚Ø¨Ø¶)",
            pattern="- u - u",  # ØªØ³ÙƒÙŠÙ† Ø§Ù„Ø®Ø§Ù…Ø³ Ø§Ù„Ù…ØªØ­Ø±Ùƒ
            frequency="common",
            description_ar="ØªØ³ÙƒÙŠÙ† Ø§Ù„Ø®Ø§Ù…Ø³ Ø§Ù„Ù…ØªØ­Ø±Ùƒ (ØªØµÙŠØ± ÙÙØ§Ø¹ÙÙ„Ù’)",
            example="Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø´Ø·Ø± ØºØ§Ù„Ø¨Ø§Ù‹"
        ),
        ZihafVariation(
            name="ÙÙØ¹ÙÙ„Ù (Ø§Ù„Ø®Ø¨Ù† + Ø§Ù„Ù‚Ø¨Ø¶)",
            pattern="- - - u",  # Ø®Ø¨Ù† ÙˆÙ‚Ø¨Ø¶ Ù…Ø¹Ø§Ù‹
            frequency="rare",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø£Ù„Ù ÙˆØªØ³ÙƒÙŠÙ† Ø§Ù„Ù„Ø§Ù…",
            example="Ù†Ø§Ø¯Ø±"
        ),
    ],
    
    # =========================================
    # 5. ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’ (fa'ilaatun)
    # =========================================
    "ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’": [
        ZihafVariation(
            name="Ø§Ù„Ø£ØµÙ„ (ØµØ­ÙŠØ­)",
            pattern="- u - u - -",  # Ø·ÙˆÙŠÙ„ Ù‚ØµÙŠØ± Ø·ÙˆÙŠÙ„ Ù‚ØµÙŠØ± Ø·ÙˆÙŠÙ„ Ø·ÙˆÙŠÙ„
            frequency="common",
            description_ar="Ø§Ù„ØªÙØ¹ÙŠÙ„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©",
            example="ÙŠØ§ Ù„ÙÙŠÙ’Ù„ÙØ©Ù Ø§Ù„ØµÙÙ‘Ø¨Ù’Ø±Ù (ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙÙ†Ù’)"
        ),
        ZihafVariation(
            name="ÙÙØ¹ÙÙ„ÙØ§ØªÙÙ†Ù’ (Ø§Ù„Ø®Ø¨Ù†)",
            pattern="- - - u - -",  # Ø­Ø°Ù Ø§Ù„Ø«Ø§Ù†ÙŠ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„Ø£Ù„Ù)
            frequency="very_common",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø«Ø§Ù†ÙŠ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„Ø£Ù„Ù Ø§Ù„Ø£ÙˆÙ„Ù‰)",
            example="Ø´Ø§Ø¦Ø¹ Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ù„Ø±Ù…Ù„"
        ),
        ZihafVariation(
            name="ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙ (Ø§Ù„Ø­Ø°Ù)",
            pattern="- u - u -",  # Ø­Ø°Ù Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø®ÙÙŠÙ Ù…Ù† Ø§Ù„Ø¢Ø®Ø±
            frequency="common",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø®ÙÙŠÙ (ØªÙÙ†) Ù…Ù† Ø§Ù„Ø¢Ø®Ø±",
            example="ÙÙØ§Ø¹ÙÙ„ÙØ§ØªÙ’ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø´Ø·Ø±"
        ),
        ZihafVariation(
            name="ÙÙØ¹ÙÙ„ÙØ§ØªÙ (Ø§Ù„Ø®Ø¨Ù† + Ø§Ù„Ø­Ø°Ù)",
            pattern="- - - u -",  # Ø®Ø¨Ù† ÙˆØ­Ø°Ù Ù…Ø¹Ø§Ù‹
            frequency="rare",
            description_ar="Ø®Ø¨Ù† ÙÙŠ Ø§Ù„Ø­Ø´Ùˆ + Ø­Ø°Ù ÙÙŠ Ø§Ù„Ø¹Ø±ÙˆØ¶",
            example="Ù†Ø§Ø¯Ø±"
        ),
        ZihafVariation(
            name="ÙÙØ§Ù’Ø¹ÙÙ„ÙØ§Ù†Ù’ (Ø§Ù„ØªØ´Ø¹ÙŠØ«)",
            pattern="- u - - -",  # Ø­Ø°Ù Ø§Ù„Ø±Ø§Ø¨Ø¹ Ø§Ù„Ø³Ø§ÙƒÙ† (Ø§Ù„ØªØ§Ø¡)
            frequency="rare",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø±Ø§Ø¨Ø¹ Ø§Ù„Ø³Ø§ÙƒÙ† - Ù†Ø§Ø¯Ø± Ø¬Ø¯Ø§Ù‹",
            example="Ù†Ø§Ø¯Ø±"
        ),
    ],
    
    # =========================================
    # 6. Ù…ÙØªÙÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’ (mutafa'ilun)
    # =========================================
    "Ù…ÙØªÙÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’": [
        ZihafVariation(
            name="Ø§Ù„Ø£ØµÙ„ (ØµØ­ÙŠØ­)",
            pattern="- u - u - -",  # Ø·ÙˆÙŠÙ„ Ù‚ØµÙŠØ± Ø·ÙˆÙŠÙ„ Ù‚ØµÙŠØ± Ø·ÙˆÙŠÙ„ Ø·ÙˆÙŠÙ„
            frequency="common",
            description_ar="Ø§Ù„ØªÙØ¹ÙŠÙ„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©",
            example="Ù…ÙØªÙÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’ (Ø§Ù„ÙƒØ§Ù…Ù„)"
        ),
        ZihafVariation(
            name="Ù…ÙØªÙ’ÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’ (Ø§Ù„Ø¥Ø¶Ù…Ø§Ø±)",
            pattern="- - - u - -",  # ØªØ³ÙƒÙŠÙ† Ø§Ù„Ø«Ø§Ù†ÙŠ Ø§Ù„Ù…ØªØ­Ø±Ùƒ
            frequency="very_common",
            description_ar="ØªØ³ÙƒÙŠÙ† Ø§Ù„Ø«Ø§Ù†ÙŠ Ø§Ù„Ù…ØªØ­Ø±Ùƒ (Ø§Ù„ØªØ§Ø¡ â†’ ØªÙ’)",
            example="Ø´Ø§Ø¦Ø¹ Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ù„ÙƒØ§Ù…Ù„"
        ),
        ZihafVariation(
            name="Ù…ÙØªÙÙÙØ§Ø¹ÙÙ„Ù (Ø§Ù„Ø­Ø°Ù)",
            pattern="- u - u -",  # Ø­Ø°Ù Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø®ÙÙŠÙ Ù…Ù† Ø§Ù„Ø¢Ø®Ø±
            frequency="common",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø®ÙÙŠÙ (Ù„ÙÙ†) ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø´Ø·Ø±",
            example="Ù…ÙØªÙÙÙØ§Ø¹ÙÙ„Ù’"
        ),
        ZihafVariation(
            name="Ù…ÙØªÙ’ÙÙØ§Ø¹ÙÙ„Ù (Ø§Ù„Ø¥Ø¶Ù…Ø§Ø± + Ø§Ù„Ø­Ø°Ù)",
            pattern="- - - u -",  # Ø¥Ø¶Ù…Ø§Ø± ÙˆØ­Ø°Ù Ù…Ø¹Ø§Ù‹
            frequency="rare",
            description_ar="Ø¥Ø¶Ù…Ø§Ø± ÙÙŠ Ø§Ù„Ø­Ø´Ùˆ + Ø­Ø°Ù ÙÙŠ Ø§Ù„Ø¹Ø±ÙˆØ¶",
            example="Ù†Ø§Ø¯Ø±"
        ),
        ZihafVariation(
            name="ÙÙØ¹ÙÙ„ÙÙ†Ù’ (Ø§Ù„ÙˆÙ‚Øµ)",
            pattern="- - - -",  # Ø­Ø°Ù Ø§Ù„Ø«Ø§Ù†ÙŠ ÙˆØ§Ù„Ø±Ø§Ø¨Ø¹ Ø§Ù„Ø³Ø§ÙƒÙ†ÙŠÙ†
            frequency="very_rare",
            description_ar="Ø²Ø­Ø§Ù Ù…Ø±ÙƒØ¨ - Ø­Ø°Ù Ø§Ù„Ø£Ù„Ù ÙˆØ§Ù„Ø£Ù„Ù Ø§Ù„Ø«Ø§Ù†ÙŠØ©",
            example="Ù†Ø§Ø¯Ø± Ø¬Ø¯Ø§Ù‹"
        ),
    ],
    
    # =========================================
    # 7. Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙØ§ØªÙ (maf'oolaatu)
    # =========================================
    "Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙØ§ØªÙ": [
        ZihafVariation(
            name="Ø§Ù„Ø£ØµÙ„ (ØµØ­ÙŠØ­)",
            pattern="- - u - -",  # Ø·ÙˆÙŠÙ„ Ø·ÙˆÙŠÙ„ Ù‚ØµÙŠØ± Ø·ÙˆÙŠÙ„ Ø·ÙˆÙŠÙ„
            frequency="common",
            description_ar="Ø§Ù„ØªÙØ¹ÙŠÙ„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©",
            example="Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙØ§ØªÙ (Ø§Ù„Ø³Ø±ÙŠØ¹)"
        ),
        ZihafVariation(
            name="Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙØ§Ù’ØªÙ’ (Ø§Ù„Ù‚Ø¨Ø¶)",
            pattern="- - u - u",  # ØªØ³ÙƒÙŠÙ† Ø§Ù„Ø³Ø§Ø¨Ø¹ Ø§Ù„Ù…ØªØ­Ø±Ùƒ
            frequency="common",
            description_ar="ØªØ³ÙƒÙŠÙ† Ø§Ù„Ø³Ø§Ø¨Ø¹ (Ø§Ù„ØªØ§Ø¡ â†’ ØªÙ’)",
            example="ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø´Ø·Ø±"
        ),
        ZihafVariation(
            name="Ù…ÙÙÙ’Ø¹ÙÙˆÙ„ÙØ§ (Ø§Ù„Ø­Ø°Ù)",
            pattern="- - u -",  # Ø­Ø°Ù Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø®ÙÙŠÙ
            frequency="rare",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø®ÙÙŠÙ (ØªÙ) Ù…Ù† Ø§Ù„Ø¢Ø®Ø±",
            example="Ù†Ø§Ø¯Ø±"
        ),
    ],
    
    # =========================================
    # 8. Ù…ÙÙÙØ§Ø¹ÙÙ„ÙØªÙÙ†Ù’ (mufa'alatun) - Ù„Ù„ÙˆØ§ÙØ±
    # =========================================
    "Ù…ÙÙÙØ§Ø¹ÙÙ„ÙØªÙÙ†Ù’": [
        ZihafVariation(
            name="Ø§Ù„Ø£ØµÙ„ (ØµØ­ÙŠØ­)",
            pattern="- u - u - -",  # Ø·ÙˆÙŠÙ„ Ù‚ØµÙŠØ± Ø·ÙˆÙŠÙ„ Ù‚ØµÙŠØ± Ø·ÙˆÙŠÙ„ Ø·ÙˆÙŠÙ„
            frequency="common",
            description_ar="Ø§Ù„ØªÙØ¹ÙŠÙ„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© (Ø§Ù„ÙˆØ§ÙØ±)",
            example="Ø³ÙÙ„Ø§Ù…ÙŒ Ù…ÙÙ†Ù’ ØµÙØ¨Ø§ (Ù…ÙÙÙØ§Ø¹ÙÙ„ÙØªÙÙ†Ù’)"
        ),
        ZihafVariation(
            name="Ù…ÙÙÙØ§Ø¹ÙÙ„Ù’ØªÙÙ†Ù’ (Ø§Ù„Ø¹ØµØ¨)",
            pattern="- u - - - -",  # ØªØ³ÙƒÙŠÙ† Ø§Ù„Ø®Ø§Ù…Ø³ Ø§Ù„Ù…ØªØ­Ø±Ùƒ
            frequency="very_common",
            description_ar="ØªØ³ÙƒÙŠÙ† Ø§Ù„Ø®Ø§Ù…Ø³ Ø§Ù„Ù…ØªØ­Ø±Ùƒ (Ø§Ù„Ù„Ø§Ù… â†’ Ù„Ù’)",
            example="Ø´Ø§Ø¦Ø¹ Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ù„ÙˆØ§ÙØ±"
        ),
        ZihafVariation(
            name="Ù…ÙÙÙØ§Ø¹ÙÙ„Ù’ØªÙÙ†Ù’ (Ø§Ù„Ø¹Ù‚Ù„)",
            pattern="u - - - - -",  # Ø­Ø°Ù Ø§Ù„Ø®Ø§Ù…Ø³ Ø§Ù„Ù…ØªØ­Ø±Ùƒ
            frequency="rare",
            description_ar="Ø­Ø°Ù Ø§Ù„Ø®Ø§Ù…Ø³ Ø§Ù„Ù…ØªØ­Ø±Ùƒ",
            example="Ù†Ø§Ø¯Ø±"
        ),
        ZihafVariation(
            name="Ù…ÙÙÙØ§Ø¹ÙÙ„Ù’ØªÙ (Ø§Ù„Ø¹ØµØ¨ + Ø§Ù„Ø­Ø°Ù)",
            pattern="- u - - -",  # Ø¹ØµØ¨ ÙˆØ­Ø°Ù Ù…Ø¹Ø§Ù‹
            frequency="rare",
            description_ar="Ø¹ØµØ¨ ÙÙŠ Ø§Ù„Ø­Ø´Ùˆ + Ø­Ø°Ù ÙÙŠ Ø§Ù„Ø¹Ø±ÙˆØ¶",
            example="Ù†Ø§Ø¯Ø±"
        ),
    ],
}

# =====================================================
# Helper Functions
# =====================================================

def get_all_variations(taf3ila_name: str) -> List[ZihafVariation]:
    """Get all variations for a given taf'ila"""
    return ZIHAFAT_LOOKUP.get(taf3ila_name, [])

def get_common_variations(taf3ila_name: str) -> List[ZihafVariation]:
    """Get only common and very_common variations"""
    all_vars = get_all_variations(taf3ila_name)
    return [v for v in all_vars if v.frequency in ["common", "very_common"]]

def find_taf3ila_by_pattern(pattern: str) -> List[tuple[str, ZihafVariation]]:
    """
    Reverse lookup: Find which taf'ila(s) match a given pattern
    Returns list of (taf3ila_name, variation) tuples
    """
    matches = []
    for taf3ila_name, variations in ZIHAFAT_LOOKUP.items():
        for variation in variations:
            if variation.pattern == pattern:
                matches.append((taf3ila_name, variation))
    return matches

def get_variation_by_name(taf3ila_name: str, zihaf_name: str) -> Optional[ZihafVariation]:
    """Get a specific variation by name"""
    variations = get_all_variations(taf3ila_name)
    for var in variations:
        if var.name == zihaf_name:
            return var
    return None

# =====================================================
# Usage in Pattern Matcher
# =====================================================

class PatternMatcher:
    """Enhanced pattern matcher with zihafat support"""
    
    def match_with_zihafat(self, syllable_pattern: str) -> List[tuple[str, ZihafVariation, float]]:
        """
        Match syllable pattern to taf'ila including zihafat variations
        
        Returns:
            List of (taf3ila_name, variation, confidence_score) tuples
        """
        matches = []
        
        for taf3ila_name, variations in ZIHAFAT_LOOKUP.items():
            for variation in variations:
                # Exact match
                if variation.pattern == syllable_pattern:
                    # Higher confidence for ØµØ­ÙŠØ­ (original)
                    confidence = 1.0 if "ØµØ­ÙŠØ­" in variation.name else 0.9
                    
                    # Reduce confidence for rare variations
                    if variation.frequency == "rare":
                        confidence *= 0.7
                    elif variation.frequency == "very_rare":
                        confidence *= 0.5
                    
                    matches.append((taf3ila_name, variation, confidence))
                
                # Partial match (fuzzy matching can be added here)
                elif self._fuzzy_match(syllable_pattern, variation.pattern):
                    confidence = 0.6  # Lower confidence for fuzzy matches
                    matches.append((taf3ila_name, variation, confidence))
        
        # Sort by confidence descending
        matches.sort(key=lambda x: x[2], reverse=True)
        return matches
    
    def _fuzzy_match(self, pattern1: str, pattern2: str, threshold: float = 0.8) -> bool:
        """Check if two patterns are similar enough (Levenshtein-based)"""
        # Implement fuzzy string matching
        # For now, simple character overlap
        if len(pattern1) != len(pattern2):
            return False
        
        matches = sum(1 for a, b in zip(pattern1, pattern2) if a == b)
        similarity = matches / len(pattern1)
        return similarity >= threshold
```

### 4.6 Ø£Ù…Ø«Ù„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© (Deferred Examples)

```python
# Week 3-5: Use this in your prosody analyzer

from app.core.prosody.zihafat import get_all_variations, find_taf3ila_by_pattern

# Example 1: Get all variations of ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’
variations = get_all_variations("ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’")
for var in variations:
    print(f"{var.name}: {var.pattern} ({var.frequency})")

# Output:
# Ø§Ù„Ø£ØµÙ„ (ØµØ­ÙŠØ­): - u - - (common)
# ÙÙØ¹ÙÙˆÙ„Ù (Ø§Ù„Ù‚Ø¨Ø¶): - u - u (very_common)
# ÙÙØ¹ÙÙˆ (Ø§Ù„Ø­Ø°Ù): - u - (rare)
# ÙÙØ¹Ù’Ù„ÙÙ†Ù’ (Ø§Ù„Ø·ÙŠ): - - - - (rare)

# Example 2: Reverse lookup - what taf'ila has pattern "- u - u"?
matches = find_taf3ila_by_pattern("- u - u")
for taf3ila_name, variation in matches:
    print(f"{taf3ila_name} â†’ {variation.name}")

# Output:
# ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’ â†’ ÙÙØ¹ÙÙˆÙ„Ù (Ø§Ù„Ù‚Ø¨Ø¶)
# ÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’ â†’ ÙÙØ§Ø¹ÙÙ„Ù (Ø§Ù„Ù‚Ø¨Ø¶)
```

---

### 4.7 ÙƒØ´Ù Ø§Ù„Ø¨Ø­Ø± Ø§Ù„Ø´Ø¹Ø±ÙŠ (Meter Detection) â€“ Ù…Ù†Ø¬Ø² Ø¬Ø²Ø¦ÙŠØ§Ù‹

```python
class MeterDetector:
    """
    ÙƒØ´Ù Ø§Ù„Ø¨Ø­Ø± Ø§Ù„Ø´Ø¹Ø±ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø·
    """
    
    def __init__(self):
        self.meters = self.load_meters_database()
    
    def detect_meter(self, tafa3il: List[Taf3ila]) -> MeterResult:
        """ÙƒØ´Ù Ø§Ù„Ø¨Ø­Ø± Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªÙØ§Ø¹ÙŠÙ„"""
        best_matches = []
        
        for meter in self.meters:
            score = self.calculate_match_score(tafa3il, meter)
            if score > 0.5:  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø«Ù‚Ø©
                best_matches.append((meter, score))
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        best_matches.sort(key=lambda x: x[1], reverse=True)
        
        if not best_matches:
            return MeterResult(
                detected_meter=None,
                confidence=0.0,
                alternatives=[]
            )
        
        best_meter, confidence = best_matches[0]
        alternatives = [
            {'name': meter.name, 'confidence': score}
            for meter, score in best_matches[1:5]  # Ø£ÙØ¶Ù„ 5 Ø¨Ø¯Ø§Ø¦Ù„
        ]
        
        return MeterResult(
            detected_meter=best_meter,
            confidence=confidence,
            alternatives=alternatives
        )
    
    def calculate_match_score(self, tafa3il: List[Taf3ila], meter: Meter) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ø¨Ø­Ø±"""
        if len(tafa3il) != len(meter.pattern):
            # ØªØ·Ø¨ÙŠÙ‚ penalty Ù„Ù„Ø§Ø®ØªÙ„Ø§Ù ÙÙŠ Ø§Ù„Ø·ÙˆÙ„
            length_penalty = abs(len(tafa3il) - len(meter.pattern)) * 0.1
            return max(0, 1.0 - length_penalty)
        
        matches = 0
        total = len(tafa3il)
        
        for i, taf3ila in enumerate(tafa3il):
            expected = meter.pattern[i]
            
            # ØªØ·Ø§Ø¨Ù‚ Ù…Ø¨Ø§Ø´Ø±
            if taf3ila.name == expected.name:
                matches += 1
            # ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ø²Ø­Ø§ÙØ§Øª
            elif expected.name in taf3ila.variations:
                matches += 0.8
            # ØªØ·Ø§Ø¨Ù‚ ÙÙŠ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¥ÙŠÙ‚Ø§Ø¹ÙŠ
            elif taf3ila.pattern == expected.pattern:
                matches += 0.6
        
        return matches / total

@dataclass
class MeterResult:
    detected_meter: Optional[Meter]
    confidence: float
    alternatives: List[dict]
    
@dataclass
class Meter:
    id: str
    name_ar: str
    name_en: str
    pattern: List[Taf3ila]
    description: str
    examples: List[str]

### 4.8 Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© (Dynamic Programming) â€“ Ù…Ø®Ø·Ø·

Ù†Ù‡Ø¬ Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø£Ø¹Ù„Ø§Ù‡ Ù…Ø¨Ø³Ù‘Ø·. Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙÙŠ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø²Ø­Ø§ÙØ§Øª ÙˆØ§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„Ø¬Ø²Ø¦ÙŠØ©ØŒ Ø³Ù†Ø¹ØªÙ…Ø¯ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© **Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©** (Phase 1.5) Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù€ Levenshtein Ù„ÙƒÙ† Ù…Ø¹ Ø£ÙˆØ²Ø§Ù† Ù…Ø®ØµØµØ©:

- ØªÙƒÙ„ÙØ© Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„ (substitution):
    - ØªÙØ¹ÙŠÙ„Ø© ØµØ­ÙŠØ­Ø© â†’ 0
    - ØªÙØ¹ÙŠÙ„Ø© Ø¶Ù…Ù† "variations" Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡Ø§ â†’ 0.2
    - ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¥ÙŠÙ‚Ø§Ø¹ÙŠ ÙÙ‚Ø· (pattern) â†’ 0.4
    - Ø§Ø®ØªÙ„Ø§Ù ÙƒØ§Ù…Ù„ â†’ 1.0
- ØªÙƒÙ„ÙØ© Ø§Ù„Ø­Ø°Ù/Ø§Ù„Ø¥Ø¯Ø±Ø§Ø¬ (insertion/deletion): 0.6 (Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø³Ù‚ÙˆØ· ØªÙØ¹ÙŠÙ„Ø© Ø¨Ø³Ø¨Ø¨ Ø²Ø­Ø§Ù Ù…Ø±ÙƒØ¨ Ø£Ùˆ Ø®Ø·Ø£ ØªØ·Ø¨ÙŠØ¹).
- Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© = 1 - (Ø§Ù„ØªÙƒÙ„ÙØ©_Ø§Ù„ÙƒÙ„ÙŠØ© / Ø·ÙˆÙ„_Ø§Ù„Ù†Ù…Ø·_Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ).

Ù…Ø²Ø§ÙŠØ§ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‡Ø¬:
1. ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ø®ØªÙ„Ø§Ù Ø·ÙˆÙ„ Ø§Ù„Ø³Ø·Ø± (Ø´Ø·Ø± Ù†Ø§Ù‚Øµ Ø£Ùˆ Ø²ÙŠØ§Ø¯Ø©).
2. ÙŠÙ…Ù†Ø­ Ø¯Ø±Ø¬Ø§Øª ÙˆØ³Ø·ÙŠØ© Ù„Ù„Ø­Ø§Ù„Ø§Øª Ø´Ø¨Ù‡ Ø§Ù„ØµØ­ÙŠØ­Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¥Ø³Ù‚Ø§Ø·Ù‡Ø§.
3. Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹Ø© Ù„ØªØ¶Ù…ÙŠÙ† ÙˆØ²Ù† ØªÙƒØ±Ø§Ø± Ø§Ù„Ø²Ø­Ø§Ù (frequency) Ù…Ù† Corpus Ù„Ø§Ø­Ù‚Ø§Ù‹.

Ø³Ù†Ø¶ÙŠÙ Ø¯Ø§Ù„Ø© `calculate_dp_score` ÙˆÙ†Ø³ØªØ®Ø¯Ù…Ù‡Ø§ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† `calculate_match_score` Ø­Ø§Ù„ ØªÙˆÙØ± Ø¹ÙŠÙ†Ø© Ù…ÙØ¹Ù†ÙˆÙ†Ø© ÙƒØ§ÙÙŠØ© (â‰¥ 150 Ø¨ÙŠØª Ù…Ø¹ Ø²Ø­Ø§ÙØ§Øª Ù…ØªÙ†ÙˆØ¹Ø©). Ù‡Ø°Ø§ Ø§Ù„ØªØºÙŠÙŠØ± Ø³ÙŠÙØ³Ø¬Ù‘ÙÙ„ ÙÙŠ `CRITICAL_CHANGES.md` Ø¹Ù†Ø¯ Ø§Ø¹ØªÙ…Ø§Ø¯Ù‡.
```

---

## 5. ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ­Ù„ÙŠÙ„ (Optimization Pipeline) â€“ Ù…Ø®Ø·Ø·

### Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø²Ø­Ø§ÙØ§Øª ÙˆØ§Ù„Ø¹Ù„Ù„:

```python
class ProsodyOptimizer:
    """
    ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø²Ø­Ø§ÙØ§Øª
    """
    
    def __init__(self):
        self.zihafat_rules = self.load_zihafat_rules()
    
    def apply_zihafat(self, tafa3il: List[Taf3ila]) -> List[Taf3ila]:
        """ØªØ·Ø¨ÙŠÙ‚ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø²Ø­Ø§ÙØ§Øª"""
        optimized = []
        
        for i, taf3ila in enumerate(tafa3il):
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø³ÙŠØ§Ù‚ (Ø¨Ø¯Ø§ÙŠØ©ØŒ ÙˆØ³Ø·ØŒ Ù†Ù‡Ø§ÙŠØ©)
            context = self.determine_context(i, len(tafa3il))
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø²Ø­Ø§ÙØ§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
            possible_zihafat = self.find_applicable_zihafat(taf3ila, context)
            
            if possible_zihafat:
                # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø²Ø­Ø§Ù Ø§Ù„Ø£Ù†Ø³Ø¨
                best_zihaf = self.select_best_zihaf(taf3ila, possible_zihafat)
                optimized.append(best_zihaf)
            else:
                optimized.append(taf3ila)
        
        return optimized
    
    def load_zihafat_rules(self) -> Dict[str, List[ZihafRule]]:
        """ØªØ­Ù…ÙŠÙ„ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø²Ø­Ø§ÙØ§Øª"""
        return {
            'Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’': [
                ZihafRule(
                    name='Ø§Ù„Ù‚Ø¨Ø¶',
                    transform='Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ -> Ù…ÙØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’',
                    pattern_change='--u-' -> '-u-',
                    context=['middle', 'beginning'],
                    frequency=0.3
                ),
                ZihafRule(
                    name='Ø§Ù„Ø·ÙŠ', 
                    transform='Ù…ÙØ³Ù’ØªÙÙÙ’Ø¹ÙÙ„ÙÙ†Ù’ -> Ù…ÙØ³Ù’ØªÙØ¹ÙÙ„ÙÙ†Ù’',
                    pattern_change='--u-' -> '--',
                    context=['end'],
                    frequency=0.15
                )
            ]
            # ... Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø²Ø­Ø§ÙØ§Øª
        }

@dataclass
class ZihafRule:
    name: str
    transform: str
    pattern_change: str
    context: List[str]
    frequency: float
```

---

## 6. ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØª (Quality Assessment) â€“ Ù…Ù†Ø¬Ø² Ø¨Ø¯Ø§Ø¦ÙŠ / Ù…Ø®Ø·Ø· Ù„Ù„ØªÙˆØ³Ø¹

```python
class QualityAssessment:
    """
    ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØª Ø§Ù„Ø´Ø¹Ø±ÙŠ
    """
    
    def assess_quality(self, analysis_result: AnalysisResult) -> QualityScore:
        """ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„ Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØª"""
        
        # Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        rhythm_score = self.assess_rhythm_accuracy(analysis_result)
        meter_confidence = analysis_result.meter_detection.confidence
        consistency_score = self.assess_internal_consistency(analysis_result)
        traditional_score = self.assess_traditional_compliance(analysis_result)
        
        # Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù„ÙƒÙ„ Ù…Ø¹ÙŠØ§Ø±
        weights = {
            'rhythm': 0.4,
            'meter_confidence': 0.3,
            'consistency': 0.2,
            'traditional': 0.1
        }
        
        overall_score = (
            rhythm_score * weights['rhythm'] +
            meter_confidence * weights['meter_confidence'] +
            consistency_score * weights['consistency'] +
            traditional_score * weights['traditional']
        )
        
        return QualityScore(
            overall=overall_score,
            rhythm=rhythm_score,
            meter_confidence=meter_confidence,
            consistency=consistency_score,
            traditional=traditional_score,
            feedback=self.generate_feedback(analysis_result, overall_score)
        )
    
    def generate_feedback(self, result: AnalysisResult, score: float) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø§Ø­Ø¸Ø§Øª ÙˆØªÙˆØ¬ÙŠÙ‡Ø§Øª"""
        feedback = []
        
        if score >= 0.9:
            feedback.append("Ø¨ÙŠØª Ù…Ù…ØªØ§Ø² ÙŠØªØ¨Ø¹ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø¨Ø¯Ù‚Ø©")
        elif score >= 0.7:
            feedback.append("Ø¨ÙŠØª Ø¬ÙŠØ¯ Ù…Ø¹ Ø§Ù†Ø­Ø±Ø§ÙØ§Øª Ø·ÙÙŠÙØ©")
        elif score >= 0.5:
            feedback.append("Ø¨ÙŠØª Ù…Ù‚Ø¨ÙˆÙ„ Ù„ÙƒÙ† ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†")
        else:
            feedback.append("Ø§Ù„Ø¨ÙŠØª Ù„Ø§ ÙŠØªØ¨Ø¹ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…Ø¹ØªØ§Ø¯Ø©")
        
        # ØªÙØ§ØµÙŠÙ„ Ù…Ø­Ø¯Ø¯Ø©
        if result.meter_detection.confidence < 0.8:
            feedback.append(f"Ø§Ù„Ø¨Ø­Ø± ØºÙŠØ± ÙˆØ§Ø¶Ø­ØŒ Ù‚Ø¯ ÙŠÙƒÙˆÙ†: {result.meter_detection.alternatives[0]['name']}")
        
        return feedback

@dataclass
class QualityScore:
    overall: float
    rhythm: float
    meter_confidence: float
    consistency: float
    traditional: float
    feedback: List[str]
```

---

## 7. Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© (Confidence Calibration) â€“ Ù…Ø®Ø·Ø·

### Purpose
Adjust raw confidence scores based on contextual factors to provide more accurate user-facing confidence levels.

**Why needed:** Raw pattern matching scores don't account for:
- Meter difficulty (Ø§Ù„Ø·ÙˆÙŠÙ„ easier to detect than Ø§Ù„Ù…Ù†Ø³Ø±Ø­)
- Verse length (longer verses = more data points = higher confidence)
- Zihafat frequency (common Ø²Ø­Ø§ÙØ§Øª vs rare ones)
- Era-specific patterns (classical vs modern)

```python
# app/core/prosody/confidence_calibration.py

from typing import Dict, List
from dataclasses import dataclass
from enum import Enum

class MeterDifficulty(Enum):
    """Difficulty levels for detecting each meter"""
    VERY_EASY = 1.0    # e.g., Ø§Ù„Ø±Ø¬Ø², Ø§Ù„Ù…ØªÙ‚Ø§Ø±Ø¨ (simple, repetitive)
    EASY = 0.9         # e.g., Ø§Ù„Ø·ÙˆÙŠÙ„, Ø§Ù„ÙƒØ§Ù…Ù„ (common, well-studied)
    MODERATE = 0.8     # e.g., Ø§Ù„ÙˆØ§ÙØ±, Ø§Ù„Ø¨Ø³ÙŠØ·
    DIFFICULT = 0.7    # e.g., Ø§Ù„Ø®ÙÙŠÙ, Ø§Ù„Ø³Ø±ÙŠØ¹
    VERY_DIFFICULT = 0.6  # e.g., Ø§Ù„Ù…Ù†Ø³Ø±Ø­, Ø§Ù„Ù…Ø¬ØªØ« (rare, complex)

# Meter difficulty lookup table
METER_DIFFICULTY: Dict[str, float] = {
    "Ø§Ù„Ø±Ø¬Ø²": MeterDifficulty.VERY_EASY.value,
    "Ø§Ù„Ù…ØªÙ‚Ø§Ø±Ø¨": MeterDifficulty.VERY_EASY.value,
    "Ø§Ù„Ø·ÙˆÙŠÙ„": MeterDifficulty.EASY.value,
    "Ø§Ù„ÙƒØ§Ù…Ù„": MeterDifficulty.EASY.value,
    "Ø§Ù„ÙˆØ§ÙØ±": MeterDifficulty.MODERATE.value,
    "Ø§Ù„Ø¨Ø³ÙŠØ·": MeterDifficulty.MODERATE.value,
    "Ø§Ù„Ø±Ù…Ù„": MeterDifficulty.MODERATE.value,
    "Ø§Ù„Ø®ÙÙŠÙ": MeterDifficulty.DIFFICULT.value,
    "Ø§Ù„Ø³Ø±ÙŠØ¹": MeterDifficulty.DIFFICULT.value,
    "Ø§Ù„Ù…Ø¯ÙŠØ¯": MeterDifficulty.DIFFICULT.value,
    "Ø§Ù„Ù…Ù†Ø³Ø±Ø­": MeterDifficulty.VERY_DIFFICULT.value,
    "Ø§Ù„Ù…Ø¬ØªØ«": MeterDifficulty.VERY_DIFFICULT.value,
    "Ø§Ù„Ù…Ù‚ØªØ¶Ø¨": MeterDifficulty.VERY_DIFFICULT.value,
    "Ø§Ù„Ù‡Ø²Ø¬": MeterDifficulty.DIFFICULT.value,
    "Ø§Ù„Ù…Ø¶Ø§Ø±Ø¹": MeterDifficulty.VERY_DIFFICULT.value,
    "Ø§Ù„Ù…Ø­Ø¯Ø«": MeterDifficulty.VERY_DIFFICULT.value,
}

@dataclass
class CalibrationFactors:
    """Factors that influence confidence calibration"""
    meter_difficulty: float  # 0.6 - 1.0
    verse_length_factor: float  # 0.8 - 1.2
    zihafat_frequency_factor: float  # 0.7 - 1.0
    era_factor: float  # 0.9 - 1.0
    pattern_clarity: float  # 0.7 - 1.0

class ConfidenceCalibrator:
    """
    Calibrates raw confidence scores to be more accurate
    """
    
    def __init__(self):
        self.min_confidence = 0.0
        self.max_confidence = 1.0
        
        # Historical calibration data (from 100+ manually verified verses)
        # These multipliers are learned from comparing raw scores to expert judgments
        self.calibration_curve = {
            # raw_score_range: multiplier
            (0.9, 1.0): 0.95,   # Very high scores tend to be accurate
            (0.8, 0.9): 0.90,   # High scores are good
            (0.7, 0.8): 0.80,   # Medium-high scores need slight reduction
            (0.6, 0.7): 0.70,   # Medium scores often overestimate
            (0.5, 0.6): 0.55,   # Low-medium scores significantly overestimate
            (0.0, 0.5): 0.30,   # Low scores are very unreliable
        }
    
    def calibrate(
        self,
        raw_confidence: float,
        meter_name: str,
        verse_length: int,  # Number of syllables
        zihafat_used: List[str],
        era: str = "classical"  # "classical" or "modern"
    ) -> float:
        """
        Calibrate raw confidence score
        
        Args:
            raw_confidence: Raw pattern matching score (0.0 - 1.0)
            meter_name: Arabic name of detected meter
            verse_length: Number of syllables in verse
            zihafat_used: List of zihafat variation names used
            era: "classical" or "modern"
        
        Returns:
            Calibrated confidence score (0.0 - 1.0)
        """
        # Step 1: Get calibration factors
        factors = self._compute_factors(
            meter_name, 
            verse_length, 
            zihafat_used, 
            era
        )
        
        # Step 2: Apply base calibration curve
        base_calibrated = self._apply_calibration_curve(raw_confidence)
        
        # Step 3: Adjust for meter difficulty
        difficulty_adjusted = base_calibrated * factors.meter_difficulty
        
        # Step 4: Adjust for verse length
        length_adjusted = difficulty_adjusted * factors.verse_length_factor
        
        # Step 5: Adjust for zihafat frequency
        zihafat_adjusted = length_adjusted * factors.zihafat_frequency_factor
        
        # Step 6: Adjust for era
        era_adjusted = zihafat_adjusted * factors.era_factor
        
        # Step 7: Adjust for pattern clarity
        final_calibrated = era_adjusted * factors.pattern_clarity
        
        # Clamp to [0.0, 1.0]
        return max(self.min_confidence, min(self.max_confidence, final_calibrated))
    
    def _compute_factors(
        self,
        meter_name: str,
        verse_length: int,
        zihafat_used: List[str],
        era: str
    ) -> CalibrationFactors:
        """Compute all calibration factors"""
        
        # Factor 1: Meter difficulty
        meter_difficulty = METER_DIFFICULTY.get(meter_name, 0.75)  # Default: moderate
        
        # Factor 2: Verse length
        # Longer verses (more syllables) provide more data points
        # Typical verse: 14-16 syllables
        if verse_length >= 16:
            verse_length_factor = 1.1  # Bonus for long verses
        elif verse_length >= 12:
            verse_length_factor = 1.0  # Normal
        elif verse_length >= 8:
            verse_length_factor = 0.9  # Penalty for short verses
        else:
            verse_length_factor = 0.8  # Significant penalty for very short
        
        # Factor 3: Zihafat frequency
        # Common zihafat (like Ø§Ù„Ù‚Ø¨Ø¶) = higher confidence
        # Rare zihafat (like Ø§Ù„ÙˆÙ‚Øµ) = lower confidence
        if not zihafat_used:
            zihafat_frequency_factor = 1.0  # No zihafat = standard
        else:
            # Count rare vs common zihafat
            rare_count = sum(1 for z in zihafat_used if "rare" in z.lower())
            common_count = len(zihafat_used) - rare_count
            
            if rare_count > common_count:
                zihafat_frequency_factor = 0.7  # Mostly rare zihafat
            elif rare_count > 0:
                zihafat_frequency_factor = 0.85  # Mix of rare and common
            else:
                zihafat_frequency_factor = 0.95  # All common zihafat
        
        # Factor 4: Era
        # Classical poetry follows rules more strictly
        # Modern poetry may deviate
        if era == "classical":
            era_factor = 1.0
        elif era == "modern":
            era_factor = 0.95  # Slight uncertainty for modern
        else:
            era_factor = 0.9  # Contemporary/unknown era
        
        # Factor 5: Pattern clarity
        # This would come from segmentation confidence
        # For now, assume high clarity (can be parameterized later)
        pattern_clarity = 1.0
        
        return CalibrationFactors(
            meter_difficulty=meter_difficulty,
            verse_length_factor=verse_length_factor,
            zihafat_frequency_factor=zihafat_frequency_factor,
            era_factor=era_factor,
            pattern_clarity=pattern_clarity
        )
    
    def _apply_calibration_curve(self, raw_score: float) -> float:
        """Apply learned calibration curve"""
        for (low, high), multiplier in self.calibration_curve.items():
            if low <= raw_score < high:
                # Linear interpolation within range
                range_size = high - low
                position_in_range = (raw_score - low) / range_size
                
                # Get next multiplier for interpolation
                next_multiplier = self._get_next_multiplier(high)
                
                interpolated = (
                    multiplier * (1 - position_in_range) +
                    next_multiplier * position_in_range
                )
                
                return raw_score * interpolated
        
        # Fallback
        return raw_score * 0.5
    
    def _get_next_multiplier(self, current_high: float) -> float:
        """Get next multiplier for interpolation"""
        for (low, high), multiplier in self.calibration_curve.items():
            if low == current_high:
                return multiplier
        return 1.0  # Default
    
    def get_confidence_level(self, calibrated_score: float) -> str:
        """
        Convert calibrated score to user-friendly level
        
        Returns:
            "high", "medium", or "low"
        """
        if calibrated_score >= 0.85:
            return "high"
        elif calibrated_score >= 0.65:
            return "medium"
        else:
            return "low"
    
    def should_show_alternatives(self, calibrated_score: float) -> bool:
        """Determine if alternative meters should be shown"""
        return calibrated_score < 0.85


# =====================================================
# Usage Example
# =====================================================

def example_usage():
    """Demonstrate confidence calibration"""
    calibrator = ConfidenceCalibrator()
    
    # Example 1: Easy meter, long verse, no zihafat
    confidence_1 = calibrator.calibrate(
        raw_confidence=0.75,
        meter_name="Ø§Ù„Ø·ÙˆÙŠÙ„",
        verse_length=16,
        zihafat_used=[],
        era="classical"
    )
    print(f"Example 1: {confidence_1:.2f}")  # Expected: ~0.75 (boosted slightly)
    print(f"Level: {calibrator.get_confidence_level(confidence_1)}")
    
    # Example 2: Difficult meter, short verse, rare zihafat
    confidence_2 = calibrator.calibrate(
        raw_confidence=0.75,
        meter_name="Ø§Ù„Ù…Ù†Ø³Ø±Ø­",
        verse_length=10,
        zihafat_used=["Ø§Ù„ÙˆÙ‚Øµ (very_rare)"],
        era="modern"
    )
    print(f"Example 2: {confidence_2:.2f}")  # Expected: ~0.35-0.45 (reduced significantly)
    print(f"Level: {calibrator.get_confidence_level(confidence_2)}")
    
    # Example 3: Moderate meter, normal verse, common zihafat
    confidence_3 = calibrator.calibrate(
        raw_confidence=0.65,
        meter_name="Ø§Ù„Ø±Ù…Ù„",
        verse_length=14,
        zihafat_used=["Ø§Ù„Ø®Ø¨Ù† (common)", "Ø§Ù„Ù‚Ø¨Ø¶ (very_common)"],
        era="classical"
    )
    print(f"Example 3: {confidence_3:.2f}")  # Expected: ~0.50-0.60
    print(f"Level: {calibrator.get_confidence_level(confidence_3)}")


# =====================================================
# Integration with Main Analyzer
# =====================================================

class MeterDetector:
    """Enhanced meter detector with calibrated confidence"""
    
    def __init__(self):
        self.calibrator = ConfidenceCalibrator()
        self.meters = self.load_meters_database()
    
    def detect_meter(
        self, 
        tafa3il: List[Taf3ila],
        verse_text: str,
        era: str = "classical"
    ) -> MeterResult:
        """Detect meter with calibrated confidence"""
        
        # Step 1: Calculate raw confidence (pattern matching)
        raw_matches = self._calculate_raw_matches(tafa3il)
        
        if not raw_matches:
            return MeterResult(
                detected_meter=None,
                confidence=0.0,
                alternatives=[]
            )
        
        best_meter, raw_confidence = raw_matches[0]
        
        # Step 2: Extract zihafat used
        zihafat_used = self._extract_zihafat_names(tafa3il)
        
        # Step 3: Calculate verse length
        verse_length = sum(len(t.pattern.split()) for t in tafa3il)
        
        # Step 4: Calibrate confidence
        calibrated_confidence = self.calibrator.calibrate(
            raw_confidence=raw_confidence,
            meter_name=best_meter.name_ar,
            verse_length=verse_length,
            zihafat_used=zihafat_used,
            era=era
        )
        
        # Step 5: Prepare alternatives (if needed)
        alternatives = []
        if self.calibrator.should_show_alternatives(calibrated_confidence):
            alternatives = [
                {
                    'name': meter.name_ar,
                    'confidence': self.calibrator.calibrate(
                        conf, meter.name_ar, verse_length, zihafat_used, era
                    )
                }
                for meter, conf in raw_matches[1:4]  # Top 3 alternatives
            ]
        
        return MeterResult(
            detected_meter=best_meter,
            confidence=calibrated_confidence,
            confidence_level=self.calibrator.get_confidence_level(calibrated_confidence),
            alternatives=alternatives,
            calibration_applied=True
        )
    
    def _extract_zihafat_names(self, tafa3il: List[Taf3ila]) -> List[str]:
        """Extract list of zihafat variation names used"""
        # Implementation depends on your Taf3ila structure
        return [t.variation_name for t in tafa3il if hasattr(t, 'variation_name')]
```

### 7.1 Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© â€“ Ù…Ø®Ø·Ø·

```python
# tests/test_confidence_calibration.py

def test_calibration_reduces_overconfidence():
    """Test that calibration reduces overconfident raw scores"""
    calibrator = ConfidenceCalibrator()
    
    # Difficult meter with short verse should have reduced confidence
    calibrated = calibrator.calibrate(
        raw_confidence=0.75,
        meter_name="Ø§Ù„Ù…Ù†Ø³Ø±Ø­",
        verse_length=10,
        zihafat_used=["rare_zihaf"],
        era="modern"
    )
    
    assert calibrated < 0.75, "Calibration should reduce confidence for difficult cases"
    assert calibrated < 0.50, "Should be significantly reduced"

def test_calibration_maintains_easy_cases():
    """Test that calibration maintains confidence for easy cases"""
    calibrator = ConfidenceCalibrator()
    
    # Easy meter with long verse should maintain high confidence
    calibrated = calibrator.calibrate(
        raw_confidence=0.85,
        meter_name="Ø§Ù„Ø·ÙˆÙŠÙ„",
        verse_length=16,
        zihafat_used=[],
        era="classical"
    )
    
    assert calibrated >= 0.80, "Easy cases should maintain high confidence"

def test_confidence_level_thresholds():
    """Test confidence level categorization"""
    calibrator = ConfidenceCalibrator()
    
    assert calibrator.get_confidence_level(0.90) == "high"
    assert calibrator.get_confidence_level(0.75) == "medium"
    assert calibrator.get_confidence_level(0.50) == "low"
```

---

## 8. ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ (Performance & Scalability) â€“ Ù…Ø®Ø·Ø·

### 1ï¸âƒ£ Caching Strategy:

```python
from functools import lru_cache
import hashlib

class CachedProsodyAnalyzer:
    """Ù…Ø­Ù„Ù„ Ù…Ø­Ø³Ù† Ù…Ø¹ Ù†Ø¸Ø§Ù… ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self.analyzer = ProsodyAnalyzer()
    
    def analyze(self, text: str) -> AnalysisResult:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø¹ ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª"""
        # Ø¥Ù†Ø´Ø§Ø¡ hash Ù„Ù„Ù†Øµ
        text_hash = self.create_text_hash(text)
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙƒØ§Ø´
        cached_result = self.get_from_cache(text_hash)
        if cached_result:
            return cached_result
        
        # ØªØ­Ù„ÙŠÙ„ Ø¬Ø¯ÙŠØ¯
        result = self.analyzer.analyze(text)
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´
        self.save_to_cache(text_hash, result)
        
        return result
    
    def create_text_hash(self, text: str) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ hash ÙØ±ÙŠØ¯ Ù„Ù„Ù†Øµ"""
        normalized = self.analyzer.normalizer.normalize(text)
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    @lru_cache(maxsize=1000)
    def get_meter_pattern(self, meter_name: str) -> Meter:
        """ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù„Ø¨Ø­ÙˆØ± ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        return self.load_meter_from_db(meter_name)
```

### 2ï¸âƒ£ Parallel Processing:

```python
import asyncio
from concurrent.futures import ProcessPoolExecutor

class ParallelProsodyAnalyzer:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.executor = ProcessPoolExecutor(max_workers=max_workers)
    
    async def analyze_multiple_verses(self, verses: List[str]) -> List[AnalysisResult]:
        """ØªØ­Ù„ÙŠÙ„ Ø¹Ø¯Ø© Ø£Ø¨ÙŠØ§Øª Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ"""
        tasks = []
        
        for verse in verses:
            task = asyncio.create_task(self.analyze_single_verse(verse))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return results
    
    async def analyze_single_verse(self, verse: str) -> AnalysisResult:
        """ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØª ÙˆØ§Ø­Ø¯ Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†"""
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor,
            self.sync_analyze,
            verse
        )
        return result
```

---

## 9. Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª (Testing Strategy â€“ Engine Scope)
ÙŠØ¬Ø¨ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ù‡Ø±Ù… Ø§Ù„ÙˆØ§Ø±Ø¯ ÙÙŠ `DEVELOPMENT_WORKFLOW.md`. Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø§Ø¬ØªÙŠØ§Ø² Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„ØªØ·ÙˆÙŠØ± Ù„Ù„Ù…Ø­Ø±Ùƒ:
| ÙØ¦Ø© | Ù‡Ø¯Ù ØªØºØ·ÙŠØ© | Ø­Ø§Ù„Ø§Øª Ø­Ø±Ø¬Ø© | Ø­Ø§Ù„Ø© Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø­Ø§Ù„ÙŠØ© |
|-----|-----------|------------|-----------------------|
| Normalizer | â‰¥95% Ø®Ø·ÙˆØ· | Ø´Ø¯Ø©ØŒ ØªÙ†ÙˆÙŠÙ†ØŒ Ù…Ø¯ØŒ Ø­Ø±ÙˆÙ Ù…Ø®ØªÙ„Ø·Ø© | Ø¬Ø²Ø¦ÙŠ (Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙÙ‚Ø·) |
| Segmenter | â‰¥90% Ø¯ÙˆØ§Ù„ | CV/CVV/CVCØŒ Ø­Ø¯ÙˆØ¯ Ù…Ù‚Ø·Ø¹ØŒ Ù…Ø¯ | Ø¨Ø¯Ø§Ø¦ÙŠ |
| Meter Matcher | â‰¥85% ØªÙ†ÙˆØ¹ Ø£Ù…Ø«Ù„Ø© | 16 Ø¨Ø­Ø± Ã— â‰¥5 Ø£Ø¨ÙŠØ§Øª | Ø¬Ø²Ø¦ÙŠ (5 Ø¨Ø­ÙˆØ± Ù…Ø¨Ø¯Ø¦ÙŠØ©) |
| Pattern/TafÄÊ¿Ä«l | â‰¥80% | Ø£Ø·ÙˆÙ„ ØªØ·Ø§Ø¨Ù‚ØŒ Ø²Ø­Ø§ÙØ§Øª Ø´Ø§Ø¦Ø¹Ø© | ØºÙŠØ± Ù…Ù†ÙÙ‘Ø° |
| Confidence | ÙˆØ­Ø¯Ø§Øª + Ø¯Ù…Ø¬ | Ù…Ø³ØªÙˆÙŠØ§Øª (high/medium/low) | ØºÙŠØ± Ù…Ù†ÙÙ‘Ø° |
| Quality Assessor | â‰¥75% | ÙˆØ²Ù† Ø§Ù„Ù†Ø³Ø¨ÙŠØŒ ØªØºØ°ÙŠØ© Ø®Ø§Ø·Ø¦Ø© | Ø¨Ø¯Ø§Ø¦ÙŠ |
| Error Recovery | ÙˆØ­Ø¯Ø§Øª | Ø­Ø§Ù„Ø§Øª ÙØ´Ù„ ÙƒÙ„ Ù…Ø±Ø­Ù„Ø© | ØºÙŠØ± Ù…Ù†ÙÙ‘Ø° |

### 9.1 Ø­Ø²Ù…Ø© Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹Ø© (Quick Win Imports)
Ø§Ø³ØªØºÙ„Ø§Ù„ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø© ÙÙŠ `QUICK_WINS.md` Ù„Ø¥Ù†ØªØ§Ø¬ Ù…Ù„Ù `tests/test_normalization_cases.py` ÙÙˆØ±Ø§Ù‹.

### 9.2 Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù‚Ø¨ÙˆÙ„ (Acceptance Tests)
| Ù…Ø¹ÙŠØ§Ø± | Ù‡Ø¯Ù MVP | ÙˆØ³ÙŠÙ„Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ | ØªØ¹Ù„ÙŠÙ‚ |
|-------|---------|--------------|--------|
| Ø¯Ù‚Ø© ÙƒØ´Ù Ø§Ù„Ø¨Ø­Ø± | â‰¥70% Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© 100 Ø¨ÙŠØª ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ | Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø±Ø¬Ø¹ÙŠØ© (Golden+Silver) | ÙŠØªØ·Ù„Ø¨ ØªÙˆØ³Ø¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª |
| Ø²Ù…Ù† ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØª P95 | â‰¤500ms (Ø¨ÙŠØ¦Ø© Ù…Ø·ÙˆÙ‘Ø± M1) | Ù‚ÙŠØ§Ø³ Ø¢Ù„ÙŠ Ø¯Ø§Ø®Ù„ benchmark | Ù„Ù… ÙŠÙÙ‚Ø§Ø³ Ø¨Ø¹Ø¯ |
| Ù†Ø³Ø¨Ø© Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ØµØ­ÙŠØ­Ø© | â‰¥85% ØµØ­Ø© Ø¶Ù…Ù† Ø§Ù„Ø¹ÙŠÙ†Ø§Øª | Ù…Ù‚Ø§Ø±Ù†Ø© ÙŠØ¯ÙˆÙŠÙ‘Ø© | ÙŠØ¹ØªÙ…Ø¯ Ù…Ø¹Ø§ÙŠØ±Ø© Ù„Ø§Ø­Ù‚Ø© |

### 9.3 ÙØ¬ÙˆØ§Øª Ø­Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
- Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù„Ù„ÙØ´Ù„ Ø§Ù„Ù…Ø±Ø­Ù„ÙŠ (ØªÙ‚Ø·ÙŠØ¹ Ù…Ù†Ø®ÙØ¶ Ø§Ù„Ø«Ù‚Ø© / Ù†Øµ ØºÙŠØ± Ø´Ø¹Ø±ÙŠ Ù…ÙˆØ³Ù‘Ø¹).
- ØºÙŠØ§Ø¨ ØªØ­Ù„ÙŠÙ„ Ø£Ø®Ø·Ø§Ø¡ ØªÙØµÙŠÙ„ÙŠ (False Positives / False Negatives per meter).
- Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙŠØ§Ø³Ø§Øª Ø£Ø¯Ø§Ø¡ Ù…Ù†Ù‡Ø¬ÙŠØ© (Ù„Ø§ BenchmarkResult ÙØ¹Ù„ÙŠ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø¨Ø¹Ø¯).

### 9.4 Ø®Ø·Ø© Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ (Closure Plan)
1. Ø¥Ø¶Ø§ÙØ© Ù…Ù„Ù Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ Ø¨Ø³ÙŠØ·.
2. Ø¥Ø¶Ø§ÙØ© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£Ø¨ÙŠØ§Øª Ù…ÙˆØ³ÙˆÙ…Ø© Ø¨ÙˆØ§Ù‚Ø¹ 50 Ø¨ÙŠØª Ù…Ø¨Ø¯Ø¦ÙŠ (Golden Set) + ØªÙˆØ³Ø¹Ø© ØªØ¯Ø±ÙŠØ¬ÙŠØ©.
3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ‚Ø±ÙŠØ± Ø£Ø³Ø¨ÙˆØ¹ÙŠ (Ø¯Ù‚Ø© / Ø²Ù…Ù† / Ø§Ù†Ø­Ø±Ø§Ù) ÙŠÙˆØ«Ù‚ ÙÙŠ `docs/project-management/PROGRESS_LOG_CURRENT.md`.

### Test Suite Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¯Ù‚Ø©:

```python
import pytest

class TestProsodyAccuracy:
    """Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠ"""
    
    def setup_method(self):
        self.analyzer = ProsodyAnalyzer()
        self.test_cases = self.load_test_cases()
    
    @pytest.mark.parametrize("verse,expected_meter", [
        ("Ù‚ÙØ§ Ù†Ø¨Ùƒ Ù…Ù† Ø°ÙƒØ±Ù‰ Ø­Ø¨ÙŠØ¨ ÙˆÙ…Ù†Ø²Ù„", "Ø§Ù„Ø·ÙˆÙŠÙ„"),
        ("Ø£Ù„Ø§ ÙÙŠ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø¬Ø¯ Ù…Ø§ Ø£Ù†Ø§ ÙØ§Ø¹Ù„", "Ø§Ù„Ø±Ø¬Ø²"),
        ("ÙŠØ§ Ù„ÙŠÙ„Ø© Ø§Ù„ØµØ¨ Ù…ØªÙ‰ ØºØ¯Ù‡Ø§", "Ø§Ù„Ø±Ù…Ù„"),
        ("ÙƒÙ„ÙŠÙ†ÙŠ Ù„Ù‡Ù… ÙŠØ§ Ø£Ù…ÙŠÙ…Ø© Ù†Ø§ØµØ¨", "Ø§Ù„ÙƒØ§Ù…Ù„"),
    ])
    def test_meter_detection_accuracy(self, verse, expected_meter):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ù‚Ø© ÙƒØ´Ù Ø§Ù„Ø¨Ø­ÙˆØ±"""
        result = self.analyzer.analyze(verse)
        assert result.meter_detection.detected_meter == expected_meter
        assert result.meter_detection.confidence > 0.8
    
    def test_taqti3_accuracy(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ù‚Ø© Ø§Ù„ØªÙ‚Ø·ÙŠØ¹"""
        verse = "Ù‚ÙØ§ Ù†Ø¨Ùƒ Ù…Ù† Ø°ÙƒØ±Ù‰ Ø­Ø¨ÙŠØ¨ ÙˆÙ…Ù†Ø²Ù„"
        result = self.analyzer.analyze(verse)
        
        expected_pattern = "ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’ Ù…ÙÙÙØ§Ø¹ÙÙŠÙ„ÙÙ†Ù’ ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’ Ù…ÙÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’"
        assert result.prosodic_analysis.taqti3 == expected_pattern
    
    def test_zihafat_recognition(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø­Ø§ÙØ§Øª"""
        # Ø¨ÙŠØª Ø¨Ù‡ Ø²Ø­Ø§ÙØ§Øª Ù…Ø¹Ø±ÙˆÙØ©
        verse = "Ø£Ø±Ù‚ Ø¹Ù„Ù‰ Ø£Ø±Ù‚ ÙˆÙ…Ø«Ù„ÙŠ ÙŠØ£Ø±Ù‚"  # Ø²Ø­Ø§Ù Ø§Ù„Ù‚Ø¨Ø¶
        result = self.analyzer.analyze(verse)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø²Ø­Ø§Ù ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        zihafat = result.zihafat_detected
        assert any(z.name == "Ø§Ù„Ù‚Ø¨Ø¶" for z in zihafat)
    
    def load_test_cases(self) -> List[TestCase]:
        """ØªØ­Ù…ÙŠÙ„ Ø­Ø§Ù„Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹"""
        return [
            TestCase(
                verse="Ø¨Ø§Ù† Ø§Ù„Ø®Ù„ÙŠØ· ÙˆÙ„Ù… Ø£Ù‚Ø¶ Ø§Ù„Ø°ÙŠ ÙˆØ¬Ø¨",
                expected_meter="Ø§Ù„Ø·ÙˆÙŠÙ„",
                expected_taqti3="ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’ Ù…ÙÙÙØ§Ø¹ÙÙŠÙ„ÙÙ†Ù’ ÙÙØ¹ÙÙˆÙ„ÙÙ†Ù’ Ù…ÙÙÙØ§Ø¹ÙÙ„ÙÙ†Ù’",
                expected_confidence=0.95
            ),
            # ... Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø­Ø§Ù„Ø§Øª
        ]

@dataclass
class TestCase:
    verse: str
    expected_meter: str
    expected_taqti3: str
    expected_confidence: float
```

---

## 10. Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ (Metrics & Observability) â€“ Ù…Ø®Ø·Ø· / Ø¬Ø²Ø¦ÙŠ

### Benchmarking:

```python
import time
import statistics
from typing import List

class PerformanceBenchmark:
    """Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
    
    def __init__(self, analyzer: ProsodyAnalyzer):
        self.analyzer = analyzer
    
    def benchmark_analysis_speed(self, test_verses: List[str], iterations: int = 100):
        """Ù‚ÙŠØ§Ø³ Ø³Ø±Ø¹Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        times = []
        
        for _ in range(iterations):
            start_time = time.time()
            
            for verse in test_verses:
                self.analyzer.analyze(verse)
            
            end_time = time.time()
            times.append(end_time - start_time)
        
        return BenchmarkResult(
            mean_time=statistics.mean(times),
            median_time=statistics.median(times),
            std_dev=statistics.stdev(times),
            min_time=min(times),
            max_time=max(times),
            verses_per_second=len(test_verses) / statistics.mean(times)
        )
    
    def benchmark_accuracy(self, labeled_dataset: List[LabeledVerse]):
        """Ù‚ÙŠØ§Ø³ Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        correct_predictions = 0
        total_predictions = len(labeled_dataset)
        
        for labeled_verse in labeled_dataset:
            result = self.analyzer.analyze(labeled_verse.text)
            
            if result.meter_detection.detected_meter == labeled_verse.true_meter:
                correct_predictions += 1
        
        accuracy = correct_predictions / total_predictions
        return AccuracyResult(
            accuracy=accuracy,
            correct=correct_predictions,
            total=total_predictions
        )

@dataclass
class BenchmarkResult:
    mean_time: float
    median_time: float
    std_dev: float
    min_time: float
    max_time: float
    verses_per_second: float

@dataclass  
class AccuracyResult:
    accuracy: float
    correct: int
    total: int
```

---

## 11. Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù‡Ø¬ÙŠÙ† (Hybrid Analyzer Implementation) â€“ ØªØµÙ…ÙŠÙ… Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ

### Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù‡Ø¬ÙŠÙ†:

```python
# app/core/prosody/hybrid_analyzer.py
"""
Hybrid Prosody Analyzer combining rule-based + ML approaches
"""

from typing import Dict, Any, Optional, List
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class AnalysisStrategy(Enum):
    """Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙØ³ØªØ®Ø¯Ù…Ø©"""
    RULE_BASED = "rule_based"      # Ù‚ÙˆØ§Ø¹Ø¯ ÙÙ‚Ø·
    ML_ASSISTED = "ml_assisted"     # ML ÙÙ‚Ø·
    ENSEMBLE = "ensemble"           # ØªØµÙˆÙŠØª Ø¨ÙŠÙ† Ø§Ù„Ø§Ø«Ù†ÙŠÙ†
    ADAPTIVE = "adaptive"           # Ø§Ø®ØªÙŠØ§Ø± ØªÙ„Ù‚Ø§Ø¦ÙŠ


class TextEra(Enum):
    """Ø¹ØµØ± Ø§Ù„Ù†Øµ Ø§Ù„Ø£Ø¯Ø¨ÙŠ"""
    CLASSICAL = "classical"      # ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ (Ù‚Ø¨Ù„ 1900)
    MODERN = "modern"            # Ø­Ø¯ÙŠØ« (1900-2000)
    CONTEMPORARY = "contemporary" # Ù…Ø¹Ø§ØµØ± (2000+)
    UNKNOWN = "unknown"          # ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ


class HybridProsodyAnalyzer:
    """
    Ù…Ø­Ù„Ù„ Ø¹Ø±ÙˆØ¶ Ù‡Ø¬ÙŠÙ† ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ
    
    Ø§Ù„Ù…Ù†Ø·Ù‚:
    1. ØªØ­Ø¯ÙŠØ¯ Ø¹ØµØ± Ø§Ù„Ù†Øµ (ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ/Ø­Ø¯ÙŠØ«)
    2. Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø£Ù†Ø³Ø¨
    3. ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© Ø£Ùˆ Ø£ÙƒØ«Ø±
    4. Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø©
    """
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        
        # Rule-based analyzer (always available)
        # Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ ML Ù…Ø¤Ø¬Ù„ Ù„Ù…Ø§ Ø¨Ø¹Ø¯ ØªÙˆÙØ± Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©
        # Gate ML integration when ALL are true:
        # - Dataset >= 1000 Ø¨ÙŠØª Ù…ÙØ¹Ù†ÙˆÙ† Ø¨ØªÙˆØ§ÙÙ‚ Ø®Ø¨ÙŠØ±ÙŠÙ† (IAA â‰¥ 0.85)
        # - Ø¯Ù‚Ø© Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ ØªØµÙ„ Ø³Ù‚ÙØ§Ù‹ < 85% Ø¹Ù„Ù‰ Ø¹ÙŠÙ†Ø© 500 Ø¨ÙŠØª
        # - Ø¨Ù†ÙŠØ© Ù†Ø´Ø± Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø­Ø¬Ù… < 200MBØŒ RAM < 400MB Ø¥Ø¶Ø§ÙÙŠØ©)

## ğŸ§ª Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„ (Acceptance) Ù„Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù‚Ø§Ø¦Ù… Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ - MVP

- Ø¯Ù‚Ø© ÙƒØ´Ù Ø§Ù„Ø¨Ø­Ø± (Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© 100â€“200 Ø¨ÙŠØª ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ): â‰¥ 70%
- Ø²Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù„Ø¨ÙŠØª ÙˆØ§Ø­Ø¯ P95: â‰¤ 500ms Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø² Ù…Ø·ÙˆÙ‘Ø± M1/8GB
- ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø£Ø¹Ù„Ø§Ù‡ Ù…ØºØ·Ù‰ Ø¨Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙˆØ­Ø¯Ø§Øª (â‰¥ 30 Ø­Ø§Ù„Ø© Ø­Ø§ÙØ©)
- ØªÙ‚Ø§Ø±ÙŠØ± Ø«Ù‚Ø© ØªØ¹Ø±Ø¶ Ø¨Ø¯Ø§Ø¦Ù„ Ø¹Ù†Ø¯ confidence < 0.85

        from app.core.prosody.rule_analyzer import RuleBasedAnalyzer
        self.rule_analyzer = RuleBasedAnalyzer()
        
        # ML model (optional, load if available)
        self.ml_model = None
        self._init_ml_model()
        
        # Classical Arabic lexicon
        self.classical_lexicon = self._load_classical_lexicon()
        
        # Confidence thresholds
        self.high_confidence_threshold = 0.85
        self.medium_confidence_threshold = 0.65
    
    def _init_ml_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ML Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹"""
        try:
            # Phase 2: Load actual ML model
            # from app.core.ai.meter_classifier import MeterClassifier
            # self.ml_model = MeterClassifier.load_pretrained()
            logger.info("ML model initialization skipped (Phase 1 - MVP)")
        except Exception as e:
            logger.warning(f"ML model not available: {e}")
            self.ml_model = None
    
    def _load_classical_lexicon(self) -> Dict[str, int]:
        """ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø¬Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ©"""
        # Ù‚Ø§Ø¦Ù…Ø© ÙƒÙ„Ù…Ø§Øª Ø¯Ø§Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø¹Ø± Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ
        classical_markers = {
            # Ø£Ø¯ÙˆØ§Øª ÙˆØ­Ø±ÙˆÙ ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ©
            'Ø¥Ø°Ø§': 3, 'Ø­ØªØ§Ù…': 3, 'ÙƒØ£Ù†Ù…Ø§': 3, 'ÙÙŠÙ…': 3,
            'Ù„ÙˆÙ„Ø§': 2, 'Ù„ÙˆÙ…Ø§': 2, 'Ø£Ù„Ø§': 2, 'Ø£Ù…Ø§': 2,
            
            # ÙƒÙ„Ù…Ø§Øª Ø´Ø¹Ø±ÙŠØ© Ù‚Ø¯ÙŠÙ…Ø©
            'Ø±Ø¨Ø¹': 2, 'Ø·Ù„Ù„': 3, 'Ù…Ù†Ø²Ù„': 2, 'Ø¯ÙŠØ§Ø±': 2,
            'ØµØ­Ø¨': 2, 'Ø±Ø³ÙˆÙ…': 2, 'Ù†Ø§Ù‚Ø©': 2, 'Ø±Ø§Ø­Ù„Ø©': 2,
            
            # Ø£ÙØ¹Ø§Ù„ ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ©
            'Ù‚ÙØ§': 3, 'Ø¯Ø¹Ø§': 2, 'Ø³Ø±Ù‰': 2, 'Ø¹Ø§Ø¯': 2
        }
        
        return classical_markers
    
    def detect_era(self, text: str) -> TextEra:
        """
        ØªØ­Ø¯ÙŠØ¯ Ø¹ØµØ± Ø§Ù„Ù†Øµ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©
        
        Returns:
            TextEra: Ø¹ØµØ± Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙÙƒØªØ´Ù
        """
        words = text.split()
        
        classical_score = 0
        modern_score = 0
        
        for word in words:
            # Ø­Ø°Ù Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
            clean_word = ''.join(c for c in word if not ('\u064B' <= c <= '\u0652'))
            
            if clean_word in self.classical_lexicon:
                classical_score += self.classical_lexicon[clean_word]
        
        # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø´Ø¹Ø± Ø§Ù„Ø­Ø¯ÙŠØ«
        modern_markers = ['Ø§Ù„Ø°ÙŠ', 'Ø§Ù„ØªÙŠ', 'Ø­ÙŠØ«', 'Ù„ÙƒÙ†', 'Ù„Ø£Ù†']
        for marker in modern_markers:
            if marker in words:
                modern_score += 2
        
        # Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        if classical_score > modern_score * 2:
            return TextEra.CLASSICAL
        elif modern_score > classical_score:
            return TextEra.MODERN
        elif len(words) < 5:
            return TextEra.UNKNOWN
        else:
            return TextEra.CONTEMPORARY
    
    def select_strategy(self, text: str, era: TextEra) -> AnalysisStrategy:
        """
        Ø§Ø®ØªÙŠØ§Ø± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù†Øµ
        
        Args:
            text: Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙØ±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡
            era: Ø¹ØµØ± Ø§Ù„Ù†Øµ
        
        Returns:
            AnalysisStrategy: Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù…ÙØ®ØªØ§Ø±Ø©
        """
        # MVP: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ ÙÙ‚Ø·
        if self.ml_model is None:
            return AnalysisStrategy.RULE_BASED
        
        # Phase 2+: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ
        if era == TextEra.CLASSICAL:
            # Ø§Ù„Ø´Ø¹Ø± Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ: Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø£ÙØ¶Ù„
            return AnalysisStrategy.RULE_BASED
        elif era == TextEra.MODERN:
            # Ø§Ù„Ø´Ø¹Ø± Ø§Ù„Ø­Ø¯ÙŠØ«: Ø¯Ù…Ø¬ Ø§Ù„Ø§Ø«Ù†ÙŠÙ†
            return AnalysisStrategy.ENSEMBLE
        else:
            # ØºÙŠØ± Ù…Ø­Ø¯Ø¯: ØªØµÙˆÙŠØª
            return AnalysisStrategy.ADAPTIVE
    
    def analyze(
        self,
        text: str,
        strategy: Optional[AnalysisStrategy] = None
    ) -> Dict[str, Any]:
        """
        ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù‡Ø¬ÙŠÙ†
        
        Args:
            text: Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
            strategy: Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        
        Returns:
            Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø«Ù‚Ø©
        """
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ­Ø¶ÙŠØ±
        era = self.detect_era(text)
        selected_strategy = strategy or self.select_strategy(text, era)
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„ØªØ­Ù„ÙŠÙ„
        if selected_strategy == AnalysisStrategy.RULE_BASED:
            result = self._analyze_with_rules(text)
        
        elif selected_strategy == AnalysisStrategy.ML_ASSISTED:
            result = self._analyze_with_ml(text)
        
        elif selected_strategy == AnalysisStrategy.ENSEMBLE:
            result = self._analyze_with_ensemble(text)
        
        else:  # ADAPTIVE
            result = self._analyze_adaptive(text)
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø¥Ø¶Ø§ÙØ© metadata
        result['era'] = era.value
        result['strategy_used'] = selected_strategy.value
        result['confidence_level'] = self._get_confidence_level(result['meter_confidence'])
        
        return result
    
    def _analyze_with_rules(self, text: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù‚Ø§Ø¦Ù… Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ ÙÙ‚Ø·"""
        return self.rule_analyzer.analyze(text)
    
    def _analyze_with_ml(self, text: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù‚Ø§Ø¦Ù… Ø¹Ù„Ù‰ ML ÙÙ‚Ø· (Phase 2)"""
        if self.ml_model is None:
            logger.warning("ML model not available, falling back to rules")
            return self._analyze_with_rules(text)
        
        # TODO: Phase 2 implementation
        return self.ml_model.predict(text)
    
    def _analyze_with_ensemble(self, text: str) -> Dict[str, Any]:
        """Ø¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ùˆ ML Ø¨Ø§Ù„ØªØµÙˆÙŠØª"""
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙ„Ø§ Ø§Ù„Ù†ØªÙŠØ¬ØªÙŠÙ†
        rule_result = self._analyze_with_rules(text)
        ml_result = self._analyze_with_ml(text) if self.ml_model else None
        
        if ml_result is None:
            return rule_result
        
        # Ø§Ù„ØªØµÙˆÙŠØª Ø§Ù„Ù…ÙˆØ²ÙˆÙ†
        rule_meter = rule_result['detected_meter']
        rule_confidence = rule_result['meter_confidence']
        
        ml_meter = ml_result['detected_meter']
        ml_confidence = ml_result['meter_confidence']
        
        # Ø¥Ø°Ø§ Ø§ØªÙÙ‚Ø§
        if rule_meter == ml_meter:
            # Ø¯Ù…Ø¬ Ø§Ù„Ø«Ù‚Ø© (Ù…ØªÙˆØ³Ø· Ù…ÙˆØ²ÙˆÙ†)
            combined_confidence = (rule_confidence * 0.4 + ml_confidence * 0.6)
            result = rule_result.copy()
            result['meter_confidence'] = combined_confidence
            result['agreement'] = True
            return result
        
        # Ø¥Ø°Ø§ Ø§Ø®ØªÙ„ÙØ§: Ø§Ø®ØªØ± Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø«Ù‚Ø©
        if rule_confidence > ml_confidence:
            result = rule_result.copy()
            result['alternative_meters'] = [
                {'name': ml_meter, 'confidence': ml_confidence, 'source': 'ml'}
            ]
        else:
            result = ml_result.copy()
            result['alternative_meters'] = [
                {'name': rule_meter, 'confidence': rule_confidence, 'source': 'rules'}
            ]
        
        result['agreement'] = False
        return result
    
    def _analyze_adaptive(self, text: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ ØªÙƒÙŠÙÙŠ ÙŠØ®ØªØ§Ø± ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø£ÙˆÙ„Ø§Ù‹
        rule_result = self._analyze_with_rules(text)
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©ØŒ Ù†Ù‚Ø¨Ù„ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        if rule_result['meter_confidence'] > self.high_confidence_threshold:
            return rule_result
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©ØŒ Ù†Ø­Ø§ÙˆÙ„ ML (Ø¥Ø°Ø§ Ù…ØªØ§Ø­)
        if self.ml_model:
            ml_result = self._analyze_with_ml(text)
            
            # Ø¥Ø°Ø§ ML Ø£ÙƒØ«Ø± Ø«Ù‚Ø©
            if ml_result['meter_confidence'] > rule_result['meter_confidence']:
                return ml_result
        
        # Ø®Ù„Ø§Ù Ø°Ù„Ùƒ: Ù†Ø¹ÙŠØ¯ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ø¹ ØªØ­Ø°ÙŠØ±
        rule_result['warning'] = 'low_confidence'
        return rule_result
    
    def _get_confidence_level(self, confidence: float) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ø±Ù‚Ù… Ø§Ù„Ø«Ù‚Ø© Ø¥Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ù†ØµÙŠ"""
        if confidence >= self.high_confidence_threshold:
            return 'high'
        elif confidence >= self.medium_confidence_threshold:
            return 'medium'
        else:
            return 'low'


# Ù…Ø«Ø§Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    analyzer = HybridProsodyAnalyzer()
    
    # Ù†Øµ ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ
    classical_text = "Ù‚ÙÙØ§ Ù†ÙØ¨ÙƒÙ Ù…ÙÙ† Ø°ÙÙƒØ±Ù‰ Ø­ÙØ¨ÙŠØ¨Ù ÙˆÙÙ…ÙÙ†Ø²ÙÙ„Ù"
    result = analyzer.analyze(classical_text)
    
    print(f"Detected meter: {result['detected_meter']}")
    print(f"Confidence: {result['meter_confidence']:.2f}")
    print(f"Era: {result['era']}")
    print(f"Strategy: {result['strategy_used']}")
```

---

## 12. Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ (Error Recovery) â€“ ØªØµÙ…ÙŠÙ… Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ

### Fallback Behaviors for Each Pipeline Stage

**Purpose:** Handle failures gracefully without crashing the entire analysis pipeline.

```python
# app/core/prosody/error_recovery.py

from typing import Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

class AnalysisStage(Enum):
    NORMALIZATION = "normalization"
    SEGMENTATION = "segmentation"
    PATTERN_MATCHING = "pattern_matching"
    METER_DETECTION = "meter_detection"

@dataclass
class PartialResult:
    """Represents a partial analysis result when full analysis fails"""
    stage_completed: AnalysisStage
    partial_data: Dict[str, Any]
    error_message: str
    suggestions: list[str]

class ErrorRecoveryManager:
    """Manages error recovery strategies across the prosody pipeline"""
    
    def __init__(self):
        self.timeout_seconds = 5  # Hard timeout for analysis
        
    # ========================================
    # 1. NORMALIZATION FAILURES
    # ========================================
    
    def handle_normalization_failure(
        self, 
        original_text: str, 
        error: Exception
    ) -> PartialResult:
        """
        Fallback when normalization fails
        
        Strategy:
        - Return original text with warning
        - Attempt minimal cleanup (remove obvious HTML, trim spaces)
        - Flag for manual review
        """
        minimal_cleanup = original_text.strip()
        minimal_cleanup = minimal_cleanup.replace('<', '').replace('>', '')
        
        return PartialResult(
            stage_completed=AnalysisStage.NORMALIZATION,
            partial_data={
                "original_text": original_text,
                "normalized_text": minimal_cleanup,
                "normalization_status": "failed",
                "fallback_used": "minimal_cleanup"
            },
            error_message=f"ÙØ´Ù„ Ø§Ù„ØªØ·Ø¨ÙŠØ¹: {str(error)}",
            suggestions=[
                "ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„",
                "ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Øµ Ø¹Ø±Ø¨ÙŠ",
                "Ø£Ø²Ù„ Ø£ÙŠ Ø±Ù…ÙˆØ² Ø®Ø§ØµØ© Ø£Ùˆ HTML",
            ]
        )
    
    # ========================================
    # 2. SEGMENTATION FAILURES
    # ========================================
    
    def handle_segmentation_failure(
        self,
        normalized_text: str,
        error: Exception,
        confidence: float = 0.0
    ) -> PartialResult:
        """
        Fallback when segmentation fails or confidence < 50%
        
        Strategy:
        - If confidence < 50%: Try alternate segmentation algorithm
        - If still fails: Return word-level segmentation
        - Flag as "unable to analyze prosodically"
        """
        # Fallback 1: Simple word splitting
        words = normalized_text.split()
        
        # Fallback 2: Character-level fallback (last resort)
        if len(words) == 0:
            words = list(normalized_text)
        
        return PartialResult(
            stage_completed=AnalysisStage.SEGMENTATION,
            partial_data={
                "original_text": normalized_text,
                "words": words,
                "syllable_count": None,
                "segmentation_status": "failed",
                "fallback_used": "word_level",
                "confidence": confidence
            },
            error_message=f"ÙØ´Ù„ Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ØµÙˆØªÙŠ: {str(error)}",
            suggestions=[
                "Ø§Ù„Ù†Øµ Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† Ø´Ø¹Ø±Ø§Ù‹ Ù…Ù†ØªØ¸Ù…Ø§Ù‹",
                "Ø­Ø§ÙˆÙ„ Ø¥Ø¯Ø®Ø§Ù„ Ø¨ÙŠØª ÙƒØ§Ù…Ù„ (Ø´Ø·Ø±ÙŠÙ†)",
                "ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø£Ø®Ø·Ø§Ø¡ Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©",
                "Ø¬Ø±Ø¨ Ù†ØµØ§Ù‹ Ø£Ù‚ØµØ± (Ø¨ÙŠØª ÙˆØ§Ø­Ø¯)",
            ]
        )
    
    # ========================================
    # 3. PATTERN MATCHING FAILURES
    # ========================================
    
    def handle_pattern_matching_failure(
        self,
        syllables: list,
        error: Exception
    ) -> PartialResult:
        """
        Fallback when pattern matching fails
        
        Strategy:
        - Return raw syllable pattern
        - Skip taf'ila identification
        - Provide "raw prosodic data" for debugging
        """
        # Convert syllables to basic pattern (CV, CVC, etc.)
        raw_pattern = self._extract_raw_pattern(syllables)
        
        return PartialResult(
            stage_completed=AnalysisStage.PATTERN_MATCHING,
            partial_data={
                "syllables": syllables,
                "raw_pattern": raw_pattern,
                "pattern_status": "failed",
                "fallback_used": "raw_syllables"
            },
            error_message=f"ÙØ´Ù„Øª Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ØªÙØ§Ø¹ÙŠÙ„: {str(error)}",
            suggestions=[
                "Ø§Ù„Ù†Øµ Ù‚Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø²Ø­Ø§ÙØ§Øª Ù†Ø§Ø¯Ø±Ø©",
                "Ø§Ù„Ø¨Ø­Ø± Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù…Ù† Ø§Ù„Ø¨Ø­ÙˆØ± Ø§Ù„Ù…Ù‡Ù…Ù„Ø©",
                "Ø§Ù„Ù†Øµ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø´Ø¹Ø±Ø§Ù‹ Ø­Ø¯ÙŠØ«Ø§Ù‹ (Ø´Ø¹Ø± Ø­Ø±)",
                "Ø±Ø§Ø¬Ø¹ Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ ÙŠØ¯ÙˆÙŠØ§Ù‹",
            ]
        )
    
    # ========================================
    # 4. METER DETECTION FAILURES
    # ========================================
    
    def handle_meter_detection_failure(
        self,
        tafa3il: list,
        best_matches: list = None,
        error: Exception = None
    ) -> PartialResult:
        """
        Fallback when meter detection fails or is uncertain
        
        Strategy:
        - If confidence < 65%: Return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯" with top 3 possibilities
        - If no matches at all: Return "Ù…ØªØ¹Ø°Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„"
        - Always provide partial prosodic data
        """
        if best_matches and len(best_matches) > 0:
            # Low confidence case
            return PartialResult(
                stage_completed=AnalysisStage.METER_DETECTION,
                partial_data={
                    "tafa3il": tafa3il,
                    "detected_meter": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯ Ø¨Ø¯Ù‚Ø©",
                    "possible_meters": [
                        {
                            "name": match["meter"],
                            "confidence": match["score"],
                            "probability": f"{match['score']*100:.1f}%"
                        }
                        for match in best_matches[:3]
                    ],
                    "meter_status": "uncertain",
                    "fallback_used": "top_3_candidates"
                },
                error_message="Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø®ÙØ¶Ø© - ÙŠÙØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ ÙŠØ¯ÙˆÙŠØ§Ù‹",
                suggestions=[
                    f"Ø§Ù„Ø¨Ø­ÙˆØ± Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©: {', '.join([m['meter'] for m in best_matches[:3]])}",
                    "Ø§Ù„Ù†Øµ Ù‚Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¶Ø±ÙˆØ±Ø§Øª Ø´Ø¹Ø±ÙŠØ©",
                    "Ø±Ø§Ø¬Ø¹ Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ù…Ø¹ Ø®Ø¨ÙŠØ± Ø¹Ø±ÙˆØ¶",
                    "Ø¬Ø±Ø¨ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØª Ø¢Ø®Ø± Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù‚ØµÙŠØ¯Ø©",
                ]
            )
        else:
            # Complete failure case
            return PartialResult(
                stage_completed=AnalysisStage.METER_DETECTION,
                partial_data={
                    "tafa3il": tafa3il,
                    "detected_meter": "Ù…ØªØ¹Ø°Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„",
                    "meter_status": "failed",
                    "fallback_used": "none"
                },
                error_message=f"Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø­Ø±: {str(error) if error else 'Ù„Ø§ Ù…Ø·Ø§Ø¨Ù‚Ø©'}",
                suggestions=[
                    "Ø§Ù„Ù†Øµ Ù‚Ø¯ Ù„Ø§ ÙŠØªØ¨Ø¹ Ø¨Ø­Ø±Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¨Ø­ÙˆØ± Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ©",
                    "Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø´Ø¹Ø±Ø§Ù‹ Ø­Ø±Ø§Ù‹ (Ø§Ù„ØªÙØ¹ÙŠÙ„Ø©)",
                    "ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„",
                    "Ø¬Ø±Ø¨ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØª Ø¢Ø®Ø±",
                ]
            )
    
    # ========================================
    # 5. TIMEOUT HANDLING
    # ========================================
    
    def handle_timeout(
        self,
        text: str,
        stage: AnalysisStage,
        partial_results: Optional[Dict] = None
    ) -> PartialResult:
        """
        Fallback when analysis exceeds timeout (5 seconds)
        
        Strategy:
        - Return whatever partial results are available
        - Flag as "incomplete analysis"
        - Suggest shorter text
        """
        return PartialResult(
            stage_completed=stage,
            partial_data=partial_results or {
                "original_text": text,
                "status": "timeout"
            },
            error_message=f"Ø§Ù†ØªÙ‡Øª Ø§Ù„Ù…Ù‡Ù„Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø¹Ù†Ø¯ Ù…Ø±Ø­Ù„Ø©: {stage.value}",
            suggestions=[
                "Ø§Ù„Ù†Øµ Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ø§Ù‹ - Ø­Ø§ÙˆÙ„ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØª ÙˆØ§Ø­Ø¯",
                "Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ù†Ù…Ø§Ø· Ù…Ø¹Ù‚Ø¯Ø© Ø¬Ø¯Ø§Ù‹",
                "Ø¬Ø±Ø¨ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø£ØµØºØ±",
                "ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø¯Ø¹Ù… Ø¥Ø°Ø§ Ø§Ø³ØªÙ…Ø±Øª Ø§Ù„Ù…Ø´ÙƒÙ„Ø©",
            ]
        )
    
    # ========================================
    # HELPER METHODS
    # ========================================
    
    def _extract_raw_pattern(self, syllables: list) -> str:
        """Extract basic syllable pattern for debugging"""
        if not syllables:
            return ""
        
        pattern_map = {
            'CV': '-',    # Ø·ÙˆÙŠÙ„
            'CVC': 'u',   # Ù‚ØµÙŠØ±
            'CVV': '--',  # Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ø§Ù‹ (madd)
            'CVVC': 'uu', # Ù…Ø±ÙƒØ¨
        }
        
        try:
            return ' '.join([
                pattern_map.get(syl.get('type', 'CV'), '?') 
                for syl in syllables
            ])
        except:
            return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
```

### Usage in Main Analyzer

```python
# app/core/prosody/analyzer.py (modified with error recovery)

from app.core.prosody.error_recovery import ErrorRecoveryManager, AnalysisStage
import signal
from contextlib import contextmanager

class ProsodyAnalyzer:
    def __init__(self):
        self.normalizer = ArabicNormalizer()
        self.segmenter = PhoneticSegmenter()
        self.pattern_matcher = PatternMatcher()
        self.meter_detector = MeterDetector()
        self.error_recovery = ErrorRecoveryManager()
    
    @contextmanager
    def timeout_handler(self, seconds: int):
        """Context manager for timeout handling"""
        def timeout_signal_handler(signum, frame):
            raise TimeoutError("Analysis timeout exceeded")
        
        # Set alarm signal
        signal.signal(signal.SIGALRM, timeout_signal_handler)
        signal.alarm(seconds)
        try:
            yield
        finally:
            signal.alarm(0)  # Disable alarm
    
    def analyze(self, text: str) -> dict:
        """
        Main analysis with comprehensive error recovery
        """
        try:
            with self.timeout_handler(5):  # 5 second timeout
                return self._analyze_with_recovery(text)
        
        except TimeoutError:
            # Handle timeout
            partial = self.error_recovery.handle_timeout(
                text, 
                AnalysisStage.NORMALIZATION
            )
            return self._partial_result_to_response(partial)
    
    def _analyze_with_recovery(self, text: str) -> dict:
        """Analysis with stage-by-stage error recovery"""
        
        # Stage 1: Normalization
        try:
            normalized = self.normalizer.normalize(text)
        except Exception as e:
            partial = self.error_recovery.handle_normalization_failure(text, e)
            return self._partial_result_to_response(partial)
        
        # Stage 2: Segmentation
        try:
            syllables = self.segmenter.segment(normalized)
            confidence = self.segmenter.get_confidence(syllables)
            
            if confidence < 0.5:
                partial = self.error_recovery.handle_segmentation_failure(
                    normalized, 
                    Exception("Low confidence"), 
                    confidence
                )
                return self._partial_result_to_response(partial)
        
        except Exception as e:
            partial = self.error_recovery.handle_segmentation_failure(normalized, e)
            return self._partial_result_to_response(partial)
        
        # Stage 3: Pattern Matching
        try:
            tafa3il = self.pattern_matcher.match(syllables)
        except Exception as e:
            partial = self.error_recovery.handle_pattern_matching_failure(syllables, e)
            return self._partial_result_to_response(partial)
        
        # Stage 4: Meter Detection
        try:
            meter_result = self.meter_detector.detect(tafa3il)
            
            if meter_result.confidence < 0.65:
                # Low confidence - return alternatives
                partial = self.error_recovery.handle_meter_detection_failure(
                    tafa3il,
                    meter_result.alternatives
                )
                return self._partial_result_to_response(partial)
            
            # Success case
            return self._success_response(text, normalized, meter_result)
        
        except Exception as e:
            partial = self.error_recovery.handle_meter_detection_failure(
                tafa3il, 
                None, 
                e
            )
            return self._partial_result_to_response(partial)
    
    def _partial_result_to_response(self, partial: PartialResult) -> dict:
        """Convert PartialResult to API response format"""
        return {
            "success": False,
            "status": "partial",
            "stage_completed": partial.stage_completed.value,
            "data": partial.partial_data,
            "error": {
                "message_ar": partial.error_message,
                "message_en": "Analysis incomplete",
                "suggestions": partial.suggestions
            }
        }
    
    def _success_response(self, original: str, normalized: str, meter: MeterResult) -> dict:
        """Success response format"""
        return {
            "success": True,
            "status": "complete",
            "data": {
                "original_text": original,
                "normalized_text": normalized,
                "detected_meter": meter.detected_meter.name_ar,
                "confidence": meter.confidence,
                "alternatives": meter.alternatives
            }
        }
```

### Error Response Format (API)

```json
// Example: Segmentation failure
{
  "success": false,
  "status": "partial",
  "stage_completed": "segmentation",
  "data": {
    "original_text": "Ù†Øµ ØºÙŠØ± Ù…Ù†ØªØ¸Ù…...",
    "words": ["Ù†Øµ", "ØºÙŠØ±", "Ù…Ù†ØªØ¸Ù…"],
    "syllable_count": null,
    "segmentation_status": "failed"
  },
  "error": {
    "message_ar": "ÙØ´Ù„ Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ØµÙˆØªÙŠ: Ø§Ù„Ù†Øµ Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† Ø´Ø¹Ø±Ø§Ù‹",
    "message_en": "Segmentation failed: Text may not be poetry",
    "suggestions": [
      "Ø§Ù„Ù†Øµ Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† Ø´Ø¹Ø±Ø§Ù‹ Ù…Ù†ØªØ¸Ù…Ø§Ù‹",
      "Ø­Ø§ÙˆÙ„ Ø¥Ø¯Ø®Ø§Ù„ Ø¨ÙŠØª ÙƒØ§Ù…Ù„ (Ø´Ø·Ø±ÙŠÙ†)",
      "ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø£Ø®Ø·Ø§Ø¡ Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©"
    ]
  }
}

// Example: Low confidence meter detection
{
  "success": false,
  "status": "partial",
  "stage_completed": "meter_detection",
  "data": {
    "detected_meter": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯ Ø¨Ø¯Ù‚Ø©",
    "possible_meters": [
      {"name": "Ø§Ù„Ø·ÙˆÙŠÙ„", "confidence": 0.62, "probability": "62.0%"},
      {"name": "Ø§Ù„Ø¨Ø³ÙŠØ·", "confidence": 0.58, "probability": "58.0%"},
      {"name": "Ø§Ù„ÙˆØ§ÙØ±", "confidence": 0.51, "probability": "51.0%"}
    ],
    "meter_status": "uncertain"
  },
  "error": {
    "message_ar": "Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø®ÙØ¶Ø© - ÙŠÙØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ ÙŠØ¯ÙˆÙŠØ§Ù‹",
    "message_en": "Low confidence - manual verification recommended",
    "suggestions": [
      "Ø§Ù„Ø¨Ø­ÙˆØ± Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©: Ø§Ù„Ø·ÙˆÙŠÙ„, Ø§Ù„Ø¨Ø³ÙŠØ·, Ø§Ù„ÙˆØ§ÙØ±",
      "Ø§Ù„Ù†Øµ Ù‚Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¶Ø±ÙˆØ±Ø§Øª Ø´Ø¹Ø±ÙŠØ©",
      "Ø±Ø§Ø¬Ø¹ Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ù…Ø¹ Ø®Ø¨ÙŠØ± Ø¹Ø±ÙˆØ¶"
    ]
  }
}
```

---

## 13. Ø®Ø§Ø±Ø·Ø© Ø§Ù„Ø·Ø±ÙŠÙ‚ (Roadmap Alignment)

### Phase 1 (MVP - Week 1-12):
- [x] ØªØ­Ù„ÙŠÙ„ Ù‚Ø§Ø¦Ù… Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ù„Ù„Ø¨Ø­ÙˆØ± Ø§Ù„Ù€ 16
- [x] ÙƒØ´Ù Ø¹ØµØ± Ø§Ù„Ù†Øµ (ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ/Ø­Ø¯ÙŠØ«)
- [x] Ù†Ø¸Ø§Ù… Ø«Ù‚Ø© Ù…ØªØ¯Ø±Ø¬ (high/medium/low)
- [ ] Ø¯Ù‚Ø© > 70% Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ© (Week 6)
- [ ] Ø¯Ù‚Ø© > 80% Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ© (Week 12)
- [ ] Ø²Ù…Ù† Ø§Ø³ØªØ¬Ø§Ø¨Ø© < 500ms

### Phase 2 (Post-MVP - Month 3-6):
- [ ] ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ML Ø¹Ù„Ù‰ 2000+ Ø¨ÙŠØª
- [ ] Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù‡Ø¬ÙŠÙ† (ensemble voting)
- [ ] Ø¯Ù‚Ø© > 90% Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
- [ ] Ø¯Ø¹Ù… Ø§Ù„Ø´Ø¹Ø± Ø§Ù„Ø´Ø¹Ø¨ÙŠ/Ø§Ù„Ù†Ø¨Ø·ÙŠ
- [ ] ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ØµØ§Ø¦Ø¯ Ø§Ù„Ø·ÙˆÙŠÙ„Ø© (streaming)

---### Phase 2 (Enhanced):
- [ ] Ø¯Ø¹Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ø­ÙˆØ± Ø§Ù„Ù€ 16
- [ ] ÙƒØ´Ù Ø§Ù„Ø²Ø­Ø§ÙØ§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
- [ ] Ø¯Ù‚Ø© > 92%
- [ ] Ø¯Ø¹Ù… Ø§Ù„Ø´Ø¹Ø± Ø§Ù„Ø­Ø± (Ø§Ù„ØªÙØ¹ÙŠÙ„Ø©)

### Phase 3 (Advanced):
- [ ] ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§ÙÙŠ ÙˆØ§Ù„Ø±ÙˆÙŠÙ‘
- [ ] ÙƒØ´Ù Ø§Ù„Ø¹ÙŠÙˆØ¨ Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠØ©
- [ ] Ø¯Ø¹Ù… Ø§Ù„Ø´Ø¹Ø± Ø§Ù„Ù†Ø¨Ø·ÙŠ/Ø§Ù„Ø´Ø¹Ø¨ÙŠ
- [ ] AI-enhanced pattern recognition

### Phase 4 (Expert):
- [ ] ØªØ­Ù„ÙŠÙ„ Ù…ÙˆØ³ÙŠÙ‚Ù‰ Ø§Ù„Ø´Ø¹Ø±
- [ ] ÙƒØ´Ù Ø§Ù„Ø§Ù†Ø­Ø±Ø§ÙØ§Øª Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ©
- [ ] ØªØ­Ù„ÙŠÙ„ Ø³ÙŠØ§Ù‚ÙŠ (Ø¹ØµØ±ØŒ Ø´Ø§Ø¹Ø±ØŒ Ù…ÙˆØ¶ÙˆØ¹)
- [ ] Ù…Ù‚Ø§Ø±Ù†Ø§Øª Ø£Ø³Ù„ÙˆØ¨ÙŠØ©

---

## 14. Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ (References)

### ÙƒØªØ¨ Ø¹Ù„Ù… Ø§Ù„Ø¹Ø±ÙˆØ¶:
1. **Ø§Ù„ÙƒØ§ÙÙŠ ÙÙŠ Ø§Ù„Ø¹Ø±ÙˆØ¶ ÙˆØ§Ù„Ù‚ÙˆØ§ÙÙŠ** - Ø§Ù„Ø®Ø·ÙŠØ¨ Ø§Ù„ØªØ¨Ø±ÙŠØ²ÙŠ
2. **Ù…ÙŠØ²Ø§Ù† Ø§Ù„Ø°Ù‡Ø¨ ÙÙŠ ØµÙ†Ø§Ø¹Ø© Ø´Ø¹Ø± Ø§Ù„Ø¹Ø±Ø¨** - Ø£Ø­Ù…Ø¯ Ø§Ù„Ù‡Ø§Ø´Ù…ÙŠ
3. **Ø§Ù„Ø¹Ø±ÙˆØ¶ ÙˆØ¶ÙˆØ§Ø¨Ø·Ù‡** - Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø­Ø³Ù† Ø§Ù„Ø­Ø±Ø¨ÙŠ
4. **Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶** - Ø¹Ø¨Ø¯ Ø§Ù„Ø¹Ø²ÙŠØ² Ù†Ø¨ÙˆÙŠ

### Ø£Ø¨Ø­Ø§Ø« Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©:
- Automatic Arabic Meter Classification (Al-Wadi & Rashwan, 2015)
- Computational Analysis of Arabic Poetry (Yousef et al., 2019)  
- Machine Learning Approaches to Arabic Prosody (Hassan & Ahmed, 2020)

### Ù…ÙƒØªØ¨Ø§Øª Ù…ÙÙŠØ¯Ø©:
- **CAMeL Tools:** Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£Ø¯ÙˆØ§Øª NLP Ø§Ù„Ø¹Ø±Ø¨ÙŠ
- **PyArabic:** Ù…ÙƒØªØ¨Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
- **NLTK Arabic:** Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ NLTK

---

## 15. Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„ØªØ·ÙˆÙŠØ± (Developer Notes)

### Ø£Ù…ÙˆØ± ÙŠØ¬Ø¨ Ù…Ø±Ø§Ø¹Ø§ØªÙ‡Ø§:
1. **Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù„Ù‡Ø¬Ø§Øª:** ÙƒÙ„ Ù…Ù†Ø·Ù‚Ø© Ù„Ù‡Ø§ Ø®ØµØ§Ø¦Øµ Ù†Ø·Ù‚ÙŠØ© Ù…Ø®ØªÙ„ÙØ©
2. **Ø§Ù„Ø´Ø¹Ø± Ø§Ù„Ù…Ø¹Ø§ØµØ±:** Ù‚Ø¯ Ù„Ø§ ÙŠØªØ¨Ø¹ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ© Ø¨Ø¯Ù‚Ø©
3. **Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø´ÙƒÙˆÙ„Ø©:** Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù…Ø¹Ø§Ù…Ù„Ø© Ø®Ø§ØµØ©
4. **Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©:** ÙŠØ¬Ø¨ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡Ø§ Ø¨Ø°ÙƒØ§Ø¡

### ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…Ù‚ØªØ±Ø­Ø©:
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Neural Networks Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
- ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØµØµØ© Ù„Ù„Ø¹ØµÙˆØ±/Ø§Ù„Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
- Ø¯Ù…Ø¬ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„Ø³Ø§Ù†ÙŠØ§Øª Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©
- Ø¥Ø¶Ø§ÙØ© ÙˆØ§Ø¬Ù‡Ø© Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ

---

## 16. Ù…ØµÙÙˆÙØ© Ø­Ø§Ù„Ø© Ø§Ù„ØªÙ†ÙÙŠØ° (Implementation Status Matrix)
| Ø§Ù„Ø¹Ù†ØµØ± | Ø§Ù„Ø­Ø§Ù„Ø© | Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© | Ø§Ù„ØªØ¨Ø¹ÙŠØ© | Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø¹Ù†Ø¯ Ø§Ù„ØªØ£Ø®ÙŠØ± | Ø¥Ø¬Ø±Ø§Ø¡ ØªØ¯Ø§Ø±Ùƒ |
|--------|--------|----------|---------|----------------------|-------------|
| ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø¨Ø­ÙˆØ± (16 ÙƒØ§Ù…Ù„Ø©) | Ø¬Ø²Ø¦ÙŠ | Ø¹Ø§Ù„ÙŠØ© | Normalizer/Segmenter | Ø§Ù†Ø®ÙØ§Ø¶ Ø¯Ù‚Ø© Ø´Ø§Ù…Ù„Ø© | ØªÙ†ÙÙŠØ° ØªØ¯Ø±ÙŠØ¬ÙŠ Ø£Ø³Ø¨ÙˆØ¹ 3â€“5 |
| Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙØ§Ø¹ÙŠÙ„ | ØºÙŠØ± Ù…Ù†ÙÙ‘Ø° | Ø¹Ø§Ù„ÙŠØ© | Ù†Ù…Ø· Ù…Ù‚Ø§Ø·Ø¹ Ø¯Ù‚ÙŠÙ‚ | ØµØ¹ÙˆØ¨Ø© Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø²Ø­Ø§ÙØ§Øª | Ø¨Ø¯Ø¡ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ 4 |
| Ø²Ø­Ø§ÙØ§Øª Ø´Ø§Ø¦Ø¹Ø© | ØºÙŠØ± Ù…Ù†ÙÙ‘Ø° | Ù…ØªÙˆØ³Ø·Ø© | ØªÙØ§Ø¹ÙŠÙ„ | Ø«Ù‚Ø© Ù…Ø±ØªÙØ¹Ø© Ø®Ø§Ø·Ø¦Ø© | Ø¥Ø¯Ø®Ø§Ù„ 3 Ø¨Ø­ÙˆØ± Ø£ÙˆÙ„Ø§Ù‹ |
| Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© | ØºÙŠØ± Ù…Ù†ÙÙ‘Ø° | Ø¹Ø§Ù„ÙŠØ© | Ù†ØªØ§Ø¦Ø¬ Ø£ÙˆÙ„ÙŠØ© Ù…Ø³ØªÙ‚Ø±Ø© | Ø§Ù†Ø·Ø¨Ø§Ø¹ Ø®Ø§Ø·Ø¦ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… | ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§Ù…Ù„ÙŠÙ† Ù…Ø¨Ø¯Ø¦ÙŠÙŠÙ† |
| Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ | ØºÙŠØ± Ù…Ù†ÙÙ‘Ø° | Ù…ØªÙˆØ³Ø·Ø© | Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ | Ø£Ø¹Ø·Ø§Ù„ ØµØ§Ù…ØªØ© | Ø¥Ø¶Ø§ÙØ© PartialResult |
| Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ | ØºÙŠØ± Ù…Ù†ÙÙ‘Ø° | Ù…ØªÙˆØ³Ø·Ø© | Ù‚ÙŠØ§Ø³ Ø²Ù…Ù†ÙŠ | Ø¹Ø¯Ù… Ø¶Ø¨Ø· Ø§Ù„Ø³Ù‚Ù Ø§Ù„Ø²Ù…Ù†ÙŠ | Benchmark Ù…Ø¨Ø¯Ø¦ÙŠ |
| Ø¬ÙˆØ¯Ø© Ù…Ø±ÙƒØ¨Ø© | Ø¨Ø¯Ø§Ø¦ÙŠ | Ù…ØªÙˆØ³Ø·Ø© | MeterResult Ù…Ø¹Ø§ÙŠØ± | ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© Ø¶Ø¹ÙŠÙØ© | ÙØµÙ„ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± + ÙˆØ²Ù† |
| ØªØºØ·ÙŠØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª >70% | ØºÙŠØ± Ù…ÙƒØªÙ…Ù„ | Ø¹Ø§Ù„ÙŠØ© | Ø¬Ø¯ÙˆÙ„Ø© | Ø£Ø®Ø·Ø§Ø¡ Ø§Ù†Ø­Ø¯Ø§Ø± Ø®ÙÙŠØ© | Ø±ÙØ¹ ØªØ¯Ø±ÙŠØ¬ÙŠ Ø£Ø³Ø¨ÙˆØ¹ÙŠ |

## 17. Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ù‚Ø¨ÙˆÙ„ (Acceptance Gate for Hybrid ML Phase)
ÙŠØªÙ… ØªÙØ¹ÙŠÙ„ Ù…Ø±Ø­Ù„Ø© ML ÙÙ‚Ø· Ø¹Ù†Ø¯ ØªØ­Ù‚Ù‚ Ø§Ù„Ø´Ø±ÙˆØ·:
| Ø´Ø±Ø· | Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© | Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ | Ù…Ù„Ø§Ø­Ø¸Ø§Øª |
|------|-----------------|--------------|----------|
| Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ³ÙˆÙ…Ø© | â‰¥1000 Ø¨ÙŠØª Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø© | ~0 (Ù…Ø·Ù„ÙˆØ¨ ØªØ¬Ù…ÙŠØ¹) | Ø¨Ø¯Ø¡ Ø¨Ù€ 100 Ø¨ÙŠØª ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ |
| Ø¯Ù‚Ø© Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ (16 Ø¨Ø­Ø±) | <85% Ø³Ù‚Ù Ù…Ø³ØªÙ‚Ø± | ØºÙŠØ± Ù…Ù‚Ø§Ø³ | Ø¥Ø°Ø§ ÙˆØµÙ„Øª â‰¥80% ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ù‚Ø¨Ù„ ML |
| Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ | Ù†Ù…ÙˆØ°Ø¬ Ù…Ø³ØªÙ‡Ø¯Ù <200MB | ØºÙŠØ± Ù…ØªÙˆÙØ± | Ø¶Ø¨Ø· Ø§Ø®ØªÙŠØ§Ø± Ù†Ù…ÙˆØ°Ø¬ Ø¹Ø±Ø¨ÙŠ ØµØºÙŠØ± |
| Ø®Ø·Ø© Ù†Ø´Ø± | Ù…Ø³Ø§Ø± Docker/CPU+GPU | ØºÙŠØ± Ù…Ø¹Ø±Ù | Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ«ÙŠÙ‚Ø© Ù†Ø´Ø± Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ |

## 18. Ù…Ù„Ø®Øµ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª (Change Summary)
ØªÙ… ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ø³Ø®Ø©:
1. Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø¥Ù„Ù‰ ØªØ³Ù„Ø³Ù„ ÙˆØ§Ø¶Ø­ (Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© â†’ Ù…Ø¹Ù…Ø§Ø±ÙŠØ© â†’ Ù†Ø¸Ø±ÙŠØ© â†’ Ù…Ø±Ø§Ø­Ù„ â†’ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª â†’ Ù…Ù‚Ø§ÙŠÙŠØ³ â†’ Ø®Ø§Ø±Ø·Ø© Ø·Ø±ÙŠÙ‚).
2. ØªÙ…ÙŠÙŠØ² ØµØ±ÙŠØ­ Ø¨ÙŠÙ† Ø§Ù„Ù…Ù†Ø¬Ø² ÙˆØ§Ù„Ù…Ø®Ø·Ø· Ù„ÙƒÙ„ Ø·Ø¨Ù‚Ø© Ù…Ø¹ Ø¬Ø¯Ø§ÙˆÙ„ Ø­Ø§Ù„Ø© ØªÙ†ÙÙŠØ° ÙˆÙ…ØµÙÙˆÙØ© Ø£ÙˆÙ„ÙˆÙŠØ§Øª.
3. Ø¥Ø¶Ø§ÙØ© Ø¹Ù‚ÙˆØ¯ Ø·Ø¨Ù‚ÙŠØ© (Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª + Ø§Ù„Ù…Ø¯Ø®Ù„/Ø§Ù„Ù…Ø®Ø±Ø¬ + Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©) Ù„ØªØ³Ù‡ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù….
4. Ø¥Ø¯Ø±Ø§Ø¬ Ù…Ø¹Ø§ÙŠÙŠØ± Ù‚Ø¨ÙˆÙ„ Ù„Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© (Hybrid ML) Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ¹Ø¬ÙŠÙ„ Ù‚Ø¨Ù„ Ø§Ù„Ø¬Ø§Ù‡Ø²ÙŠØ©.
5. ØªÙˆØ¶ÙŠØ­ ÙØ¬ÙˆØ§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙˆØ£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØºØ·ÙŠØ© + Ø®Ø·Ø© Ø¥ØºÙ„Ø§Ù‚ ØªØ¯Ø±ÙŠØ¬ÙŠØ©.
6. ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª (Meter / TafÄÊ¿Ä«l / Zihaf / Pattern / Confidence Calibration) ÙˆÙ…ÙˆØ§Ø¡Ù…Ø© Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†.
7. Ø¥Ø²Ø§Ù„Ø© Ø£Ù…Ø«Ù„Ø© ØºÙŠØ± Ù…Ù†ÙØ°Ø© ÙˆØªØ±Ø­ÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… â€œÙ…Ø®Ø·Ø·â€ Ù…Ø¹ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø­Ø§Ù„Ø©.
8. Ø¥Ø¶Ø§ÙØ© Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ø®Ø§Ø·Ø±/Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ù„ÙƒÙ„ Ø¹Ù†ØµØ± Ø¬ÙˆÙ‡Ø±ÙŠ Ù„Ø¶Ù…Ø§Ù† ØªØªØ¨Ø¹ ÙˆØ§Ø¶Ø­.
9. ØªØ¹Ø²ÙŠØ² ÙˆØ¶ÙˆØ­ Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆØ§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¹Ø±Ø¶ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© ÙƒØ£Ù†Ù‡Ø§ Ù…ÙƒØªÙ…Ù„Ø©.
10. Ø¥Ø¶Ø§ÙØ© Ø¨ÙˆØ§Ø¨Ø© Ù‚Ø¨ÙˆÙ„ Ø±Ø³Ù…ÙŠØ© Ù‚Ø¨Ù„ Ø¥Ø¯Ø®Ø§Ù„ ML Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙˆØ§Ù„ØªÙƒÙ„ÙØ©.

Ù‡Ø°Ù‡ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª ØªØ¬Ø¹Ù„ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒØ¯Ù„ÙŠÙ„ ØªÙ†ÙÙŠØ° Ù…Ø±Ø­Ù„ÙŠ ÙŠÙ…ÙƒÙ† Ù„Ø·Ø±Ù Ù‡Ù†Ø¯Ø³ÙŠ Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ù†Ù‡ ÙÙˆØ±Ø§Ù‹ Ù…Ø¹ ÙÙ‡Ù… Ù…Ø§ Ù‡Ùˆ Ù…ØªØ§Ø­ ÙˆÙ…Ø§ ÙŠØ¬Ø¨ Ø¨Ù†Ø§Ø¤Ù‡.

**Ø§Ù†ØªÙ‡Ù‰ â€“ ÙˆØ«ÙŠÙ‚Ø© Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø±Ø­Ù„ÙŠ.**