# Evaluation Protocol for 100% Meter Detection Accuracy

**Version:** 1.0
**Date:** 2025-11-12
**Purpose:** Define comprehensive testing protocol to certify 100% accuracy across all 20 Arabic meters
**Scope:** From current 19/20 meters (95%) to complete 20/20 coverage (100%)

---

## ğŸ¯ Executive Summary

**Current Status:**
- âœ… 182 verses tested (v0.102 golden set)
- âœ… 100% accuracy on 19/20 meters
- âŒ Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ: 0 test verses (blind spot)

**Target Status:**
- ğŸ¯ 200+ verses tested (expanded golden set)
- ğŸ¯ 100% accuracy on 20/20 meters
- ğŸ¯ Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ: 15 test verses minimum
- ğŸ¯ External expert validation
- ğŸ¯ Published certification report

**Evaluation Philosophy:**
> **"Not merely 100% on tested verses, but provable confidence that all 20 meters are comprehensively covered with representative samples across difficulty levels and variants."**

---

## ğŸ“Š Evaluation Framework

### 1. Multi-Dimensional Accuracy Metrics

**Primary Metrics (MUST achieve 100%):**
1. **Per-Meter Accuracy**
   - Calculation: `correct_detections_per_meter / total_verses_per_meter`
   - Requirement: 100% for EACH of 20 meters independently
   - No averaging allowed

2. **Overall Accuracy**
   - Calculation: `total_correct / total_verses`
   - Requirement: 100% (200/200 or higher)

3. **Confusion Matrix Purity**
   - All off-diagonal elements MUST be 0
   - No misclassifications between any meter pairs

**Secondary Metrics (Diagnostic):**
4. **Macro F1-Score**
   - Unweighted average F1 across all 20 meters
   - Ensures rare meters aren't penalized
   - Target: 1.00

5. **Weighted F1-Score**
   - Weighted by meter frequency in classical poetry
   - Should also be 1.00 given primary metrics

6. **Confidence Calibration**
   - Mean confidence: â‰¥ 0.90
   - Minimum confidence: â‰¥ 0.80
   - Confidence-accuracy correlation: â‰¥ 0.95

---

### 2. Coverage Requirements

**Minimum Test Set Size Per Meter:**

| Meter Tier | Frequency | Min Verses | Recommended | Rationale |
|------------|-----------|------------|-------------|-----------|
| **Tier 1** (9 meters) | 85% of poetry | 10 | 15-30 | High coverage needed for common cases |
| **Tier 2** (2 meters) | 10% of poetry | 8 | 10-15 | Adequate variant coverage |
| **Tier 3** (5 meters) | 5% of poetry | 10 | 15 | Higher per-meter to cover edge cases |
| **Variants** (4 forms) | Variable | 5 | 5-10 | Sufficient for variant validation |

**Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ Specific:**
- **Minimum:** 15 verses (higher than typical Tier 3 due to historical gaps)
- **Distribution:**
  - Canonical: 5 verses
  - With khabn: 4 verses
  - With á¸¥adhf: 3 verses
  - With qaá¹£r: 2 verses
  - Edge cases: 1 verse
- **Difficulty:**
  - Easy: 3 (20%)
  - Medium: 6 (40%)
  - Hard: 6 (40%)

---

### 3. Stratified Testing Protocol

#### 3.1 Difficulty Stratification

**Tier 1 - Easy (Target: 20% of test set)**
- Canonical forms (no ziá¸¥ÄfÄt or Ê¿ilal)
- Clear, unambiguous meter patterns
- High-confidence detections expected (â‰¥ 0.95)

**Tier 2 - Medium (Target: 50% of test set)**
- 1-2 ziá¸¥ÄfÄt applied
- Common variants
- Standard classical poetry
- Confidence: 0.85-0.95

**Tier 3 - Hard (Target: 30% of test set)**
- 3+ ziá¸¥ÄfÄt applied
- Rare Ê¿ilal combinations
- Boundary cases with other meters
- Ambiguous contexts
- Confidence: 0.80-0.90

**Rationale:**
- Real-world poetry is NOT all "easy" - must handle complexity
- Hard cases prevent overfitting to simple patterns
- 30% hard cases ensures robustness

---

#### 3.2 Variant Coverage

**Required Variants Per Meter:**
Each meter must have examples of:
1. Base canonical form (ØµØ­ÙŠØ­)
2. Common ziá¸¥ÄfÄt variants (at least 2 types)
3. Common Ê¿ilal variants (at least 2 types)
4. Combined transformations (ziá¸¥Äf + Ê¿ilal)

**Example - Ø§Ù„Ø·ÙˆÙŠÙ„:**
- âœ… Canonical: ÙØ¹ÙˆÙ„Ù† Ù…ÙØ§Ø¹ÙŠÙ„Ù† (Ã—4)
- âœ… With qabd: ÙØ¹ÙˆÙ„Ù† Ù…ÙØ§Ø¹Ù„Ù†
- âœ… With kaff: ÙØ¹ÙˆÙ„ Ù…ÙØ§Ø¹ÙŠÙ„Ù†
- âœ… With á¸¥adhf: ÙØ¹ÙˆÙ„Ù† Ù…ÙØ§Ø¹ÙŠ (final)
- âœ… Combined: ÙØ¹ÙˆÙ„ Ù…ÙØ§Ø¹Ù„Ù† Ù…ÙØ§Ø¹ÙŠ (qabd + kaff + á¸¥adhf)

---

#### 3.3 Confusion Matrix Testing

**High-Risk Pairs (MUST test explicitly):**

| Meter 1 | Meter 2 | Confusion Risk | Required Boundary Tests |
|---------|---------|----------------|-------------------------|
| **Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ** | **Ø§Ù„Ù…ØªÙ‚Ø§Ø±Ø¨** | **CRITICAL** | 5+ verses at exact boundary |
| **Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ** | **Ø§Ù„Ø±Ø¬Ø²** | **HIGH** | 4+ verses differentiating |
| **Ø§Ù„ÙƒØ§Ù…Ù„** | **Ø§Ù„Ø±Ø¬Ø²** | MEDIUM | 3+ verses |
| **Ø§Ù„Ø¨Ø³ÙŠØ·** | **Ø§Ù„Ù…Ø¯ÙŠØ¯** | MEDIUM | 3+ verses |
| **Ø§Ù„Ø³Ø±ÙŠØ¹** | **Ø§Ù„Ù…Ù†Ø³Ø±Ø­** | LOW | 2+ verses |

**Boundary Test Protocol:**
1. Create verses that COULD match both meters
2. Verify detector chooses correct meter
3. Document distinguishing features
4. Confidence for correct meter must be > incorrect meter
5. Example: If verse is Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ, detector must return:
   - Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ: 0.92 confidence (1st)
   - Ø§Ù„Ù…ØªÙ‚Ø§Ø±Ø¨: 0.45 confidence (2nd or lower)

---

### 4. Test Set Composition

#### 4.1 Current Golden Set (v0.102)

```
Total verses: 182
Meters covered: 19/20 (95%)
Missing: Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ (0 verses)

Distribution:
- Tier 1 meters: 150 verses (82%)
- Tier 2 meters: 11 verses (6%)
- Tier 3 meters: 13 verses (7%)
- Variants: 8 verses (4%)
```

**Gap Analysis:**
- âŒ Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ: Completely missing
- âš ï¸ Ø§Ù„Ø³Ø±ÙŠØ¹: Only 1 verse (need 9 more)
- âš ï¸ Ø§Ù„Ù…Ø¯ÙŠØ¯: Only 1 verse (need 7 more)
- âš ï¸ Tier 3 meters: Under-represented (need 40+ more)

---

#### 4.2 Target Golden Set (v0.103+)

```
Total verses: 250 (target)
Meters covered: 20/20 (100%)

Additions needed:
+ 15 Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ verses (PRIORITY)
+  9 Ø§Ù„Ø³Ø±ÙŠØ¹ verses
+  7 Ø§Ù„Ù…Ø¯ÙŠØ¯ verses
+  5 Ø§Ù„Ù…Ù†Ø³Ø±Ø­ verses
+  2 Ø§Ù„Ù…Ø¬ØªØ« verses
+  2 Ø§Ù„Ù…Ù‚ØªØ¶Ø¨ verses
+  2 Ø§Ù„Ù…Ø¶Ø§Ø±Ø¹ verses
+  5 Variant expansions
+  5 Boundary confusion tests
= ~52 verses minimum
```

**Recommended: 250 verses total (182 + 68 new)**
- Provides buffer for edge cases
- Comprehensive variant coverage
- Statistical confidence

---

### 5. Evaluation Execution Protocol

#### 5.1 Testing Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. GOLDEN SET PREPARATION           â”‚
â”‚    - Finalize 15 Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ verses   â”‚
â”‚    - Add to golden_set_v0_103.jsonl â”‚
â”‚    - Validate schema compliance     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. PRE-TEST VALIDATION              â”‚
â”‚    - Run schema validator           â”‚
â”‚    - Check for duplicates           â”‚
â”‚    - Verify taqá¹­Ä«Ê¿ annotations      â”‚
â”‚    - Ensure all metadata present    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. AUTOMATED EVALUATION             â”‚
â”‚    - Run: test_golden_set_v2.py     â”‚
â”‚    - Detector: BahrDetectorV2       â”‚
â”‚    - Record all results             â”‚
â”‚    - Generate confusion matrix      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. RESULTS ANALYSIS                 â”‚
â”‚    - Per-meter accuracy             â”‚
â”‚    - Overall accuracy               â”‚
â”‚    - Confidence distributions       â”‚
â”‚    - Failure analysis (if any)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. EXPERT VALIDATION (Blind)        â”‚
â”‚    - 2+ external prosodists         â”‚
â”‚    - Annotate same test set         â”‚
â”‚    - Compare with detector output   â”‚
â”‚    - Calculate inter-rater Îº        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. STATISTICAL VALIDATION           â”‚
â”‚    - Chi-square test (no meter bias)â”‚
â”‚    - Bootstrap confidence intervals â”‚
â”‚    - Cross-validation (if applicable)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. CERTIFICATION REPORT             â”‚
â”‚    - Document all metrics           â”‚
â”‚    - Expert attestations            â”‚
â”‚    - Methodology description        â”‚
â”‚    - Dataset publication            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 5.2 Evaluation Script Enhancement

**Current:** `test_golden_set_v2.py`
**Enhancements Needed:**

1. **Confusion Matrix Generation**
```python
def generate_confusion_matrix(results):
    """
    Create 20Ã—20 confusion matrix
    Rows: True meter
    Cols: Detected meter
    All off-diagonal elements MUST be 0 for certification
    """
    # Implementation
```

2. **Per-Meter Detailed Report**
```python
def generate_meter_report(meter_id, results):
    """
    Detailed report for each meter:
    - Accuracy (must be 100%)
    - Verse count and difficulty distribution
    - Confidence statistics (mean, min, max, std)
    - Variant coverage
    - Edge case results
    """
    # Implementation
```

3. **Statistical Tests**
```python
def chi_square_meter_bias_test(results):
    """
    Test null hypothesis: No significant difference in
    accuracy across meters (should NOT reject - all meters equal)
    """
    # Implementation

def bootstrap_confidence_intervals(results, n_iterations=10000):
    """
    Calculate 95% CI for overall accuracy
    Target: [99.5%, 100%] for 250 verses
    """
    # Implementation
```

4. **Certification Checklist**
```python
def certification_checklist(results):
    """
    Auto-generate pass/fail checklist:
    [ ] Overall accuracy = 100%
    [ ] Per-meter accuracy = 100% (all 20 meters)
    [ ] Confusion matrix diagonal (no off-diagonal)
    [ ] Mean confidence â‰¥ 0.90
    [ ] Min confidence â‰¥ 0.80
    [ ] All meters have â‰¥ min verses
    [ ] Expert validation Îº â‰¥ 0.85
    [ ] No statistical meter bias
    """
    # Implementation
```

---

### 6. Expert Validation Protocol

#### 6.1 External Expert Review

**Purpose:** Independent verification that detector accuracy is genuine, not overfitted

**Protocol:**
1. **Recruit 2-3 prosody experts** NOT involved in golden set creation
2. **Blind annotation:** Provide verses WITHOUT gold labels or detector outputs
3. **Independent work:** Experts don't see each other's annotations initially
4. **Consensus meeting:** Resolve disagreements for final labels
5. **Comparison:** Calculate agreement between experts and detector

**Metrics:**
- **Inter-expert agreement (Îº):** â‰¥ 0.85 (shows test set is reliable)
- **Detector-expert agreement (Îº):** â‰¥ 0.90 (shows detector matches experts)
- **Disagreement analysis:** Document ALL cases where detector â‰  expert consensus

**Acceptance Criteria:**
- If Îº(detector, experts) â‰¥ 0.90 AND detector accuracy = 100%: âœ… PASS
- If Îº < 0.90: Investigate disagreements; may indicate test set issues
- If detector wrong but experts agree: ğŸ› BUG in detector
- If experts disagree (Îº < 0.85): âš ï¸ Verse is ambiguous; may need removal

---

#### 6.2 Expert Attestation Form

**Required for certification:**

```
EXPERT VALIDATION ATTESTATION

I, [Expert Name], [Credentials], hereby attest that:

1. I have independently reviewed [N] verses from the BAHR golden set
2. I have compared my prosodic analysis with the BahrDetectorV2 output
3. The detector achieved [X]% agreement with my expert annotations
4. I have reviewed all disagreement cases and confirm:
   - [ ] Detector was correct, I made an error
   - [ ] Detector was incorrect (documented in error report)
   - [ ] Verse is genuinely ambiguous (documented)
5. To the best of my professional judgment, the BAHR detector demonstrates
   gold-standard accuracy on Arabic meter detection across all 20 classical meters

Specifically for Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ:
6. I have reviewed [N] Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ verses in the test set
7. All [N] verses are authentic Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ (not misclassified other meters)
8. The detector correctly identified [N/N = 100%] of these verses

Date: _______________
Signature: _______________
Affiliation: _______________
```

**Minimum:** 2 signed attestations required for certification

---

### 7. Certification Criteria

#### 7.1 Technical Requirements (All MUST Pass)

| # | Criterion | Threshold | Status |
|---|-----------|-----------|--------|
| 1 | Overall accuracy | 100% (no errors) | â¬œ |
| 2 | Per-meter accuracy | 100% on each of 20 meters | â¬œ |
| 3 | Confusion matrix | All off-diagonal = 0 | â¬œ |
| 4 | Mean confidence | â‰¥ 0.90 | â¬œ |
| 5 | Min confidence | â‰¥ 0.80 | â¬œ |
| 6 | Test set size | â‰¥ 200 verses | â¬œ |
| 7 | Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ coverage | â‰¥ 15 verses | â¬œ |
| 8 | All meters coverage | â‰¥ 5 verses each | â¬œ |
| 9 | Difficulty distribution | 20% easy, 50% medium, 30% hard | â¬œ |
| 10 | Variant coverage | All major variants tested | â¬œ |

#### 7.2 Validation Requirements (All MUST Pass)

| # | Criterion | Threshold | Status |
|---|-----------|-----------|--------|
| 11 | Inter-annotator Îº (experts) | â‰¥ 0.85 | â¬œ |
| 12 | Detector-expert Îº | â‰¥ 0.90 | â¬œ |
| 13 | Expert attestations | â‰¥ 2 signed | â¬œ |
| 14 | Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ expert validation | â‰¥ 2 prosodists confirm all 15 verses | â¬œ |
| 15 | External blind review | â‰¥ 1 independent review | â¬œ |

#### 7.3 Documentation Requirements (All MUST Be Complete)

| # | Deliverable | Status |
|---|-------------|--------|
| 16 | Test methodology document | â¬œ |
| 17 | Golden set with full metadata (JSONL) | â¬œ |
| 18 | Evaluation results report (JSON + PDF) | â¬œ |
| 19 | Confusion matrix visualization | â¬œ |
| 20 | Expert attestation forms (signed) | â¬œ |
| 21 | Statistical analysis report | â¬œ |
| 22 | Reproducible test harness (code) | â¬œ |
| 23 | Dataset publication (Zenodo/HuggingFace) | â¬œ |

**Certification Status:** âœ… CERTIFIED only when ALL 23 criteria are met

---

### 8. Reporting & Documentation

#### 8.1 Evaluation Report Structure

**File:** `BAHR_100_PERCENT_CERTIFICATION_REPORT.pdf`

**Sections:**

```
1. EXECUTIVE SUMMARY
   - Current accuracy: 100%
   - Meters tested: 20/20
   - Total verses: [N]
   - Certification date
   - Certifying experts

2. METHODOLOGY
   - Test set construction
   - Stratified sampling approach
   - Validation protocols
   - Expert review process

3. RESULTS
   3.1 Overall Metrics
       - Accuracy: 100%
       - Macro F1: 1.00
       - Weighted F1: 1.00
   3.2 Per-Meter Results
       - Table: 20 rows, each meter 100%
   3.3 Confusion Matrix
       - 20Ã—20 matrix (all off-diagonal = 0)
   3.4 Confidence Analysis
       - Distribution plots
       - Statistics per meter

4. Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ VALIDATION (Special Section)
   4.1 Historical Context
   4.2 Corpus Sourcing
   4.3 Expert Validation
   4.4 Disambiguation from Ø§Ù„Ù…ØªÙ‚Ø§Ø±Ø¨/Ø§Ù„Ø±Ø¬Ø²
   4.5 Results: 15/15 correct (100%)

5. EXPERT VALIDATION
   5.1 Expert Qualifications
   5.2 Blind Annotation Protocol
   5.3 Inter-Annotator Agreement (Îº = X.XX)
   5.4 Detector-Expert Agreement (Îº = X.XX)
   5.5 Attestation Letters (Appendix)

6. STATISTICAL ANALYSIS
   6.1 Chi-Square Test (no meter bias)
   6.2 Bootstrap Confidence Intervals
   6.3 Cross-Validation Results (if applicable)

7. DATASET DESCRIPTION
   7.1 Composition by Meter
   7.2 Composition by Difficulty
   7.3 Composition by Era
   7.4 Variant Coverage
   7.5 Source Distribution

8. REPRODUCIBILITY
   8.1 Test Harness Usage
   8.2 Dataset Access (Zenodo DOI)
   8.3 Code Repository (GitHub)
   8.4 Dependencies and Versions

9. LIMITATIONS & FUTURE WORK
   9.1 Current Scope
   9.2 Out-of-Scope Patterns
   9.3 Recommendations for Extensions

10. CERTIFICATION STATEMENT
    "The BAHR detection engine (BahrDetectorV2) has achieved
     gold-standard accuracy on all 20 classical Arabic meters
     as validated by independent expert prosodists and comprehensive
     testing protocols."

11. APPENDICES
    A. Complete Test Set (JSONL)
    B. Expert Attestation Forms
    C. Confusion Matrix (Full)
    D. Per-Verse Results
    E. Statistical Test Output
```

---

#### 8.2 Dataset Publication

**Platform:** Zenodo (DOI registration) + HuggingFace (accessibility)

**Files to Publish:**
1. `golden_set_v0_103_complete.jsonl` - Full test set
2. `golden_set_schema.json` - Data schema
3. `golden_set_metadata.json` - Statistics
4. `BAHR_100_PERCENT_CERTIFICATION_REPORT.pdf` - Full report
5. `test_golden_set_v2.py` - Evaluation script
6. `evaluation_results.json` - Raw results
7. `confusion_matrix.csv` - Confusion matrix data
8. `README.md` - Dataset documentation

**License:** CC BY-SA 4.0 (allows academic use with attribution)

**Citation:**
```bibtex
@dataset{bahr_golden_set_2025,
  author = {BAHR Detection Engine Team},
  title = {Gold-Standard Arabic Poetry Meter Test Set: 20 Meters},
  year = {2025},
  publisher = {Zenodo},
  doi = {10.5281/zenodo.XXXXXXX},
  url = {https://doi.org/10.5281/zenodo.XXXXXXX}
}
```

---

### 9. Failure Response Protocol

**IF any criterion is NOT met:**

#### 9.1 Accuracy < 100%

**Response:**
1. **Identify failures:** Which verses were misclassified?
2. **Root cause analysis:**
   - Detector bug?
   - Incorrect gold label?
   - Genuinely ambiguous verse?
3. **Categorize errors:**
   - Type 1: Detector error â†’ Fix detection logic
   - Type 2: Gold label error â†’ Correct annotation, re-test
   - Type 3: Ambiguous verse â†’ Remove from golden set OR resolve with expert panel
4. **Re-test after fixes**
5. **Document all changes**

---

#### 9.2 Low Expert Agreement (Îº < 0.85)

**Response:**
1. **Calibration session:** Convene experts to discuss disagreements
2. **Reference materials:** Provide classical prosody references
3. **Consensus building:** Resolve ambiguous cases
4. **Verse quality review:** Remove genuinely ambiguous verses
5. **Re-annotation:** Repeat with calibrated experts
6. **Accept:** Îº â‰¥ 0.85 OR acknowledge limitations in ambiguous cases

---

#### 9.3 Insufficient Coverage

**Response:**
1. **Identify gaps:** Which meters/variants/difficulty levels under-represented?
2. **Targeted sourcing:** Use MUTADARIK_CORPUS_SOURCING_GUIDE.md
3. **Add verses:** Expand golden set
4. **Re-test:** Full evaluation with expanded set

---

### 10. Timeline & Milestones

| Week | Milestone | Deliverable |
|------|-----------|-------------|
| 1-2 | Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ corpus sourcing | 20 candidate verses |
| 3-4 | Expert annotation & validation | 15 verified Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ verses |
| 5 | Golden set expansion | v0.103 with 200+ verses |
| 6 | Automated evaluation | Full test run, results analysis |
| 7 | Expert blind validation | External review, attestations |
| 8 | Statistical analysis | Chi-square, bootstrap, reports |
| 9 | Documentation | Certification report draft |
| 10 | Peer review | External feedback, revisions |
| 11 | Dataset publication | Zenodo/HuggingFace upload |
| 12 | Certification | Final sign-off, announcement |

**Total:** ~12 weeks from start to certification

---

## âœ… Success Indicators

**We will have achieved 100% certification when:**

1. âœ… BahrDetectorV2 correctly classifies ALL 200+ verses (100%)
2. âœ… Each of 20 meters individually shows 100% accuracy
3. âœ… Confusion matrix has ZERO off-diagonal elements
4. âœ… 2+ independent prosody experts attest to accuracy
5. âœ… Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ specifically validated with 15 verses
6. âœ… Statistical tests confirm no meter bias
7. âœ… Complete documentation published
8. âœ… Dataset openly available with DOI
9. âœ… Reproducible test harness provided
10. âœ… Peer review (if applicable) completed

---

**Document Owner:** BAHR Detection Engine Team
**Status:** Ready for implementation
**Next Steps:** Execute Ø§Ù„Ù…ØªØ¯Ø§Ø±Ùƒ corpus sourcing â†’ annotation â†’ evaluation
